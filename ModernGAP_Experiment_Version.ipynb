{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lua-Nova/Modern-GAP-GNN/blob/new/ModernGAP_Experiment_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFQCNsDWU5mf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  #NVIDIA GPU version\n",
        "  %pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "  %pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "  %pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "  %pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "  %pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "else:\n",
        "  #CPU version\n",
        "  %pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric==2.0.0 -f https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
        "%pip uninstall pyvacy  --y\n",
        "%pip install pyvacy\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fBxqGGgYU6k-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import Sequential, GCNConv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb1hVZklFoDP"
      },
      "source": [
        "## Encoder Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1ScntAVtWL3M"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(11)\n",
        "# create classes for layers that are used a lot to avoid repeating code\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  # e.g. dimensions = [50,40,30,20]\n",
        "    def __init__(self, dimensions):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        layers = []\n",
        "        for i in range(len(dimensions)-1):\n",
        "          layers.append(nn.Linear(dimensions[i], dimensions[i+1]))\n",
        "          layers.append(nn.SELU(inplace=True))\n",
        "\n",
        "        self.linear_selu_stack = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_selu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF264FWv0pJI"
      },
      "source": [
        "## PMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OMzLWqwF03Uu"
      },
      "outputs": [],
      "source": [
        "class AggregationModule(nn.Module):\n",
        "  edge_index = None\n",
        "\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "class PMA(AggregationModule):\n",
        "    # A - adjacency matrix     TODO: this should not be given to the module itself, it should access it in training (or from the graph dataset)\n",
        "    # num_hops - the number of hops covered by this GNN\n",
        "    def __init__(self, num_hops, sigma):\n",
        "        super().__init__()\n",
        "        self.num_hops = num_hops\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TEMP SOLUTION\n",
        "        if AggregationModule.edge_index is None:\n",
        "          raise RuntimeError(\"Set AggregationModule.edge_index [TEMP SOLUTION] before running\")\n",
        "        edge_index = AggregationModule.edge_index\n",
        "        A = get_adjacency_matrix(edge_index, x.size(dim=0))\n",
        "        out = [torch.nn.functional.normalize(x, dim=1)]\n",
        "        for k in range(self.num_hops):\n",
        "            aggr = torch.mm(torch.transpose(A, 0, 1), out[-1].to(device))\n",
        "            noised = aggr + torch.normal(torch.zeros(aggr.size()), std=self.sigma).to(device)\n",
        "            normalized = torch.nn.functional.normalize(noised, dim=1)\n",
        "            out.append(normalized)\n",
        "        return torch.stack(out)\n",
        "        # return torch.nn.functional.normalize(x, dim=1)\n",
        "\n",
        "class PMAT(AggregationModule):\n",
        "    # def __init__(self, num_hops, transform_dimensions):\n",
        "    def __init__(self, num_hops, encoding_dimensions, sigma):\n",
        "        super().__init__()\n",
        "        self.num_hops = num_hops\n",
        "        self.sigma = sigma\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # self.transforms = nn.ModuleList()\n",
        "        self.attentions = nn.ModuleList()\n",
        "        for i in range(num_hops):\n",
        "          # self.transforms.append(nn.Linear(*transform_dimensions)) # Only 1 layer transformation\n",
        "          # self.attentions.append(MLP([2*transform_dimensions[-1], 1])) # Attention mechanism takes 2 encodings and outputs 1 weight\n",
        "          # TODO: Figure out if we want a transformer?\n",
        "          self.attentions.append(MLP([2*encoding_dimensions, 1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TEMP SOLUTION\n",
        "        if AggregationModule.edge_index is None:\n",
        "          raise RuntimeError(\"Set AggregationModule.edge_index [TEMP SOLUTION] before running\")\n",
        "        edge_index = AggregationModule.edge_index\n",
        "        out = [torch.nn.functional.normalize(x, dim=1)]\n",
        "        for k in range(self.num_hops):\n",
        "            # Do we need to do a transform? I reckon we can use raw encoding and aggregate according to attention (and then the Classification module)\n",
        "            # can handle how the aggregations get transformed\n",
        "            # h = self.transforms[k](out[-1])\n",
        "            h = out[-1]\n",
        "            e_values = self.attentions[k](h[edge_index.T].reshape(edge_index.size(dim=1), 2*h.size(dim=1))) # DPSGD to guarantee DP attention training\n",
        "            # we have to use Sigmoid because if we use Softmax, removing an edge will change the weight of all other edges in the neighbourhood\n",
        "            alpha_values = self.sigmoid(e_values)\n",
        "            alpha = torch.sparse_coo_tensor(edge_index,\n",
        "                                            alpha_values.reshape(edge_index.size(dim=1)),\n",
        "                                            (x.size(dim=0), x.size(dim=0)),\n",
        "                                            dtype=torch.float).transpose(0, 1)\n",
        "\n",
        "            aggr = torch.sparse.mm(alpha, h)\n",
        "            # Might need to not use \"transforms\" and instead do raw aggregations like the original PMA\n",
        "            # aggr = torch.mm(self.A, out[-1]) \n",
        "            # noised = aggr # TODO: add noise # Gaussian mechanism to guarantee DP for neighbourhood aggregation\n",
        "            noised = aggr + torch.normal(torch.zeros(aggr.size()), std=self.sigma).to(device)\n",
        "            normalized = torch.nn.functional.normalize(noised, dim=1)\n",
        "            out.append(normalized)\n",
        "        return torch.stack(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty_dao27oEAB"
      },
      "source": [
        "## Classification Module\n",
        "NOTE: \n",
        "\n",
        "MLP base: The first MLP in the cassification module. \n",
        "\n",
        "MLP head: The last MLP and takes the combined output of all MLP base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7dLm3Q3HofT6"
      },
      "outputs": [],
      "source": [
        "class Classification(nn.Module):\n",
        "    # num_hops - the number of hops covered by this GNN\n",
        "    # encoder_dimensions - the MLP dimensions of each base MLP\n",
        "    # head_dimensions - the dimensions of the head MLP\n",
        "    def __init__(self, num_hops, encoder_dimensions, head_dimensions):\n",
        "        super().__init__()\n",
        "        self.base_mlps = nn.ModuleList()\n",
        "        self.num_hops = num_hops\n",
        "        if encoder_dimensions:\n",
        "          for i in range(num_hops+1):\n",
        "              self.base_mlps.append(MLP(encoder_dimensions))\n",
        "        self.head_mlp = MLP(head_dimensions) # TODO: should this be softmax? I think we add a softmax for classification tasks. We can test if it works better\n",
        "    \n",
        "    def forward(self, cache):\n",
        "        # forward through bases\n",
        "        out = []\n",
        "        for i in range(self.num_hops+1):\n",
        "          if self.base_mlps:\n",
        "            encoding = self.base_mlps[i](cache[i,:,:])\n",
        "            out.append(encoding) # add corresponding encoding\n",
        "          else:\n",
        "            out.append(cache[i, :, :])\n",
        "        # combine (use concatenation)\n",
        "        combined_x = torch.cat(out, dim=1)\n",
        "        # forward through head\n",
        "        return self.head_mlp(combined_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "evGOLmy-UrMs"
      },
      "outputs": [],
      "source": [
        "class GAP(nn.Module):\n",
        "  # encoder - pretrained encoder module\n",
        "  # pma - PMA module\n",
        "  # classification - classification module\n",
        "  def __init__(self, encoder, pma, classification): # TODO: decide whether we should recieve the models as parameters\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.encoder.requires_grad=False\n",
        "    self.pma = pma\n",
        "    self.classification = classification\n",
        "\n",
        "  def forward(self, x):\n",
        "    # initial node encoding\n",
        "    x_encoded = self.encoder(x)\n",
        "    # aggregation module\n",
        "    cache = self.pma(x_encoded) \n",
        "    # classification\n",
        "    return self.classification(cache) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WebCohAOIX0P"
      },
      "source": [
        "##Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "G_Pf4P_bIZ5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d327b660-d6b9-4fcd-cf5b-3048376d8089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon: 4.00, Delta: 0.50, Sigma: 0.53\n"
          ]
        }
      ],
      "source": [
        "node_level = True\n",
        "\n",
        "# Edge level DP\n",
        "agg_epsilon, agg_delta, epsilon_1, epsilon_5, pmat_epsilon, pmat_delta = 4, 0.5, 4, 4, 4, 0.5\n",
        "K_hop = 1\n",
        "agg_sigma = 1 / np.max(np.roots([K_hop/2, np.sqrt(2*K_hop*np.log(1/agg_delta)), -agg_epsilon]))\n",
        "# Node level DP\n",
        "if (node_level):\n",
        "  pass\n",
        "  # How do we calculate this?\n",
        "data = \"reddit\"\n",
        "batch_size = 32\n",
        "\n",
        "print(f\"Epsilon: {agg_epsilon:>0.2f}, Delta: {agg_delta:>0.2f}, Sigma: {agg_sigma:>0.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "XDJP3FhOi_Ty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IPX-rb88ukrc"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "# this method partitions based on nodes (so edges between splits are not used)\n",
        "def train_test_split(dataset, test_ratio):\n",
        "    X, y, edge_index= dataset.x, dataset.y, dataset.edge_index\n",
        "    shuffle_ordering = torch.randperm(X.size(dim=0))\n",
        "\n",
        "    edge_mapping = torch.zeros(X.size(dim=0), dtype=torch.long)\n",
        "    edge_mapping[shuffle_ordering] = torch.arange(X.size(dim=0))\n",
        "\n",
        "    X = X[shuffle_ordering]\n",
        "    y = y[shuffle_ordering]\n",
        "    edge_index = edge_mapping[edge_index]\n",
        "\n",
        "    mask = torch.zeros(X.size(dim=0), dtype=torch.bool)\n",
        "    train_slice = int((1-test_ratio)*X.size(dim=0))\n",
        "    mask[:train_slice] = True\n",
        "\n",
        "    X_train = X[mask]\n",
        "    X_test = X[~mask]\n",
        "\n",
        "    y_train = y[mask]\n",
        "    y_test = y[~mask]\n",
        "\n",
        "    edge_index_train = edge_index[:, torch.logical_and(*mask[edge_index])]\n",
        "    edge_index_test = edge_index[:, torch.logical_and(*~mask[edge_index])] - train_slice\n",
        "\n",
        "    return Data(x=X_train, y=y_train, edge_index=edge_index_train), \\\n",
        "           Data(x=X_test, y=y_test, edge_index=edge_index_test)\n",
        "\n",
        "\n",
        "# returns filtered edge index, first removes edges that have removed src or dst nodes, then shifts indices of remained src/dst nodes\n",
        "def filter_edge_index(edge_index, filter):\n",
        "\n",
        "    node_indices = torch.arange(filter.size(dim=0))[filter]\n",
        "    edge_mapping = torch.zeros(filter.size(dim=0), dtype=torch.long)\n",
        "    edge_mapping[node_indices] = torch.arange(node_indices.size(dim=0))\n",
        "\n",
        "\n",
        "    edge_index = edge_index.to(torch.long)\n",
        "    edge_filter = torch.logical_and(*filter[edge_index])\n",
        "    return edge_mapping[edge_index[:, edge_filter]]\n",
        "\n",
        "def add_edge_to_low_degree_nodes(dataset, low_degree_threshold):\n",
        "    X, y, edge_index = dataset.x, dataset.y, dataset.edge_index\n",
        "\n",
        "    # get low degree nodes\n",
        "    A = get_adjacency_matrix(edge_index, X.size(dim=0))\n",
        "    sums = torch.sparse.sum(A, dim=1).to_dense()\n",
        "    mask = sums < low_degree_threshold\n",
        "    \n",
        "    # get edge_index mask for neighbours of low degree nodes\n",
        "    filter = mask[edge_index[0, :]]\n",
        "    low_degree_edges_index = edge_index[:, filter]\n",
        "    low_degree_A = get_adjacency_matrix(low_degree_edges_index, X.size(dim=0))\n",
        "\n",
        "    # get 1-hop neighbours and add to A \n",
        "    # NOTE: without sampling (just adds all 1-hop neighbours)\n",
        "    one_hop_low_degree_A = torch.sparse.mm(low_degree_A, A)\n",
        "    new_edge_index = torch.add(one_hop_low_degree_A, A).coalesce().indices()\n",
        "    return Data(x=X, y=y, edge_index=new_edge_index)\n",
        "    \n",
        "\n",
        "def prepare_dataset(dataset, threshold):\n",
        "    X, y, edge_index = dataset.x, dataset.y, dataset.edge_index\n",
        "\n",
        "    # remove labels with less examples than threshold\n",
        "    index_map = torch.zeros(y.size())\n",
        "    included_classes = y.unique(return_counts=True)[1] >= threshold\n",
        "    filter = included_classes[y]\n",
        "    # remap labels (i.e. if they were 0-8 and we remove 4 labels, new labels should be between 0 and 4)\n",
        "    label_mapping = torch.zeros(included_classes.size(dim=0), dtype=torch.long)\n",
        "    label_mapping[included_classes] = torch.arange(torch.count_nonzero(included_classes))\n",
        "\n",
        "    y = label_mapping[y[filter]].to(torch.long)\n",
        "    X = X[filter]\n",
        "\n",
        "    # remove edges that had their nodes removed\n",
        "    edge_index = filter_edge_index(edge_index, filter)\n",
        "\n",
        "    return Data(x=X, y=y, edge_index=edge_index)\n",
        "\n",
        "# make sparse adjacency matrix, A\n",
        "def get_adjacency_matrix(edge_index, num_nodes):\n",
        "    values = torch.ones(edge_index.size(dim=1), dtype = torch.int).to(device)\n",
        "    A = torch.sparse_coo_tensor(edge_index, values, (num_nodes, num_nodes), dtype=torch.float)\n",
        "    return A\n",
        "\n",
        "def standardization(train_dataset, test_dataset):\n",
        "    X = train_dataset.x\n",
        "    means = X.mean(dim=0, keepdim=True)\n",
        "    stds = X.std(dim=0, keepdim=True)\n",
        "    X_train = (X - means) / stds\n",
        "    X_test = (test_dataset.x - means) / stds\n",
        "    return Data(x=X_train, y=train_dataset.y, edge_index=train_dataset.edge_index), Data(x=X_test, y=test_dataset.y, edge_index=test_dataset.edge_index)\n",
        "\n",
        "def add_self_edges(dataset):\n",
        "    X = dataset.x\n",
        "    self_edges = torch.stack((torch.arange(X.size(dim=0)), torch.arange(X.size(dim=0))))\n",
        "    edge_index = torch.cat((dataset.edge_index, self_edges), dim=1)\n",
        "    return Data(x=X, y=dataset.y, edge_index=edge_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7oEXJ63oyl-"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Reddit\n",
        "dataset = Reddit('.')[0]\n",
        "# prepare dataset by removing classes that have less than 1000 examples\n",
        "dataset = prepare_dataset(dataset, 10000)\n",
        "dataset = add_self_edges(dataset)\n",
        "# get num classes\n",
        "num_classes = torch.unique(dataset.y).size(dim=0)\n",
        "\n",
        "# train/test split\n",
        "train_dataset, test_dataset = train_test_split(dataset, 0.2)\n",
        "print(train_dataset.edge_index.size(dim=1))\n",
        "# train_dataset = add_edge_to_low_degree_nodes(train_dataset, 10)\n",
        "print(train_dataset.edge_index.size(dim=1))\n",
        "# test_dataset = add_edge_to_low_degree_nodes(test_dataset, 10)\n",
        "train_dataset, test_dataset = standardization(train_dataset, test_dataset)"
      ],
      "metadata": {
        "id": "JrAKSDBl03mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f0e8a1-b2c5-47cb-802c-399200ebd8a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.dgl.ai/dataset/reddit.zip\n",
            "Extracting ./raw/reddit.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29495772\n",
            "29495772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "29Swawd9we8f"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "X_train, y_train, edge_index_train = train_dataset.x, train_dataset.y, train_dataset.edge_index\n",
        "X_test, y_test, edge_index_test = test_dataset.x, test_dataset.y, test_dataset.edge_index\n",
        "\n",
        "# using large number like 10,000 so that all neighbours are sampled \n",
        "# I don't like how it samples, so I'm just gonna sample everything\n",
        "train_loader = NeighborLoader(train_dataset, num_neighbors=[X_train.size(dim=0)]*K_hop, \n",
        "                              batch_size=batch_size, shuffle=True)\n",
        "test_loader = NeighborLoader(test_dataset, num_neighbors=[X_test.size(dim=0)]*K_hop, \n",
        "                             batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Md2Ds3q3CY"
      },
      "source": [
        "## Train/Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nu0AtfY7qzba"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "def train(batch, model, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  X, y = batch.x.to(device), batch.y.to(device)\n",
        "  AggregationModule.edge_index = batch.edge_index.to(device)\n",
        "  # compute prediction error\n",
        "  pred = model(X)\n",
        "  loss = loss_fn(pred[:batch.batch_size], y[:batch.batch_size])\n",
        "  # backpropagation\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# test batch\n",
        "def batch_test(batch, split, model, loss_fn, wordy=False):\n",
        "    size = batch.batch_size\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.inference_mode():\n",
        "        X, y = batch.x.to(device), batch.y.to(device)\n",
        "        AggregationModule.edge_index = batch.edge_index.to(device)\n",
        "        pred = model(X)\n",
        "        test_loss = loss_fn(pred[:batch.batch_size], y[:batch.batch_size]).item()\n",
        "        correct = (pred[:batch.batch_size].argmax(1) == y[:batch.batch_size]).type(torch.float).sum().item() / size\n",
        "    if wordy:\n",
        "      print(f\"{split.title()} Error: \\n Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f}\")\n",
        "    return test_loss, correct\n",
        "\n",
        "# test\n",
        "def test(loader, split, model, loss_fn):\n",
        "    size = len(loader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for batch in loader:\n",
        "        batch_loss, batch_correct = batch_test(batch, split, model, loss_fn)\n",
        "        test_loss += batch_loss\n",
        "        correct += batch_correct\n",
        "    correct /= size\n",
        "    test_loss /= size\n",
        "    print(f\"{split.title()} Error: \\n Avg Accuracy: {(100*correct):>0.1f}%, Avg Loss: {test_loss:>8f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFvQEYa3j3O7"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XixvkAQpqcFM"
      },
      "source": [
        "Encoder Design\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9ZT6RQbkiTbj"
      },
      "outputs": [],
      "source": [
        "# encoder\n",
        "dimensions = [602, 300, 60]\n",
        "encoder_model = nn.Sequential(\n",
        "    MLP(dimensions),\n",
        "    nn.Linear(dimensions[-1], num_classes),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Jk2-tbqLer"
      },
      "source": [
        "Encoder Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HYGNjZgqo7k",
        "outputId": "ebdcb08c-c809-4216-a3af-222270c6d91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Error: \n",
            " Accuracy: 75.0%, Loss: 1.535050\n",
            "Train Error: \n",
            " Accuracy: 81.2%, Loss: 1.484142\n",
            "Train Error: \n",
            " Accuracy: 75.0%, Loss: 1.534483\n",
            "Train Error: \n",
            " Accuracy: 65.6%, Loss: 1.594603\n",
            "Train Error: \n",
            " Accuracy: 62.5%, Loss: 1.659892\n",
            "Train Error: \n",
            " Accuracy: 81.2%, Loss: 1.513257\n",
            "Train Error: \n",
            " Accuracy: 81.2%, Loss: 1.456602\n",
            "Train Error: \n",
            " Accuracy: 81.2%, Loss: 1.452872\n",
            "Train Error: \n",
            " Accuracy: 68.8%, Loss: 1.612048\n",
            "Train Error: \n",
            " Accuracy: 68.8%, Loss: 1.578732\n",
            "Train Error: \n",
            " Accuracy: 81.2%, Loss: 1.491779\n",
            "Train Error: \n",
            " Accuracy: 68.8%, Loss: 1.548485\n",
            "Train Error: \n",
            " Accuracy: 71.9%, Loss: 1.558250\n",
            "Train Error: \n",
            " Accuracy: 71.9%, Loss: 1.577147\n",
            "Train Error: \n",
            " Accuracy: 62.5%, Loss: 1.618584\n",
            "Train Error: \n",
            " Accuracy: 68.8%, Loss: 1.595579\n",
            "Train Error: \n",
            " Accuracy: 90.6%, Loss: 1.381599\n",
            "Train Error: \n",
            " Accuracy: 75.0%, Loss: 1.525755\n",
            "Train Error: \n",
            " Accuracy: 65.6%, Loss: 1.594417\n",
            "Train Error: \n",
            " Accuracy: 84.4%, Loss: 1.439731\n",
            "Test Error: \n",
            " Avg Accuracy: 75.4%, Avg Loss: 1.522556\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "encoder_model = encoder_model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(encoder_model.parameters(), lr=1e-3)\n",
        "\n",
        "# if node_level:\n",
        "#   optimizer = op.optimizers.optimizer.DPOptimizer(\n",
        "#       # TODO: Fill out these parameters '?'\n",
        "#       optimizer=optimizer,\n",
        "#       noise_multiplier=?,\n",
        "#       max_grad_norm=?\n",
        "#   )\n",
        "\n",
        "for t in range(200):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, encoder_model, loss_fn, optimizer)\n",
        "    if (t + 1) % 20 == 0:\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", encoder_model, loss_fn, True)\n",
        "test(test_loader, \"TEST\", encoder_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "encoder = encoder_model[0]\n",
        "encoder.requires_grad=False\n",
        "\n",
        "# for name, param in encoder_model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name, param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PMAT Training"
      ],
      "metadata": {
        "id": "WOoJSnrTSVWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvacy import optim, analysis\n",
        "# PMAT\n",
        "pmat_model = nn.Sequential(\n",
        "    encoder,\n",
        "    PMAT(K_hop, 60, agg_sigma),\n",
        "    Classification(K_hop, [], [(K_hop+1)*60, num_classes])\n",
        ")\n",
        "pmat_model = pmat_model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(encoder_model.parameters(), lr=1e-3)\n",
        "# TODO: Not allowed dyanmic batch_size for DPSGD, our batches are edge-wise\n",
        "# so they should have fixed batch_size!\n",
        "optimizer = optim.DPAdam(\n",
        "    l2_norm_clip=1.0,\n",
        "    noise_multiplier=1.0,\n",
        "    batch_size=batch_size,\n",
        "    params=pmat_model.parameters(),\n",
        "    lr=0.5e-1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)"
      ],
      "metadata": {
        "id": "LKxUUyl_SW-i"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_epochs = 0\n",
        "for t in range(10000):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, pmat_model, loss_fn, optimizer)\n",
        "    edge_epochs += batch_size / X_train.size(dim=0)\n",
        "    epsilon = analysis.moments_accountant(X_train.size(dim=0), batch_size, \n",
        "                                          1.0, edge_epochs, pmat_delta)\n",
        "    if (t + 1) % 500 == 0:\n",
        "      print(\"Epoch:\", edge_epochs)\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", pmat_model, loss_fn, True)\n",
        "      print(\"Optimizer Achieves ({:>0.1f}, {})-DP\".format(epsilon, pmat_delta))\n",
        "      print(\"LR:\", scheduler.get_last_lr()[0])\n",
        "    scheduler.step()\n",
        "    if epsilon >= pmat_epsilon:\n",
        "      break\n",
        "test(test_loader, \"TEST\", pmat_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "pmat = pmat_model[1]\n",
        "pmat.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpsiubMeSoTE",
        "outputId": "e7f1dc59-8f24-44ce-dc14-73e9c81a7d16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0.17095478246004095\n",
            "Train Error: \n",
            " Accuracy: 68.8%, Loss: 1.039748\n",
            "Optimizer Achieves (0.0, 0.5)-DP\n",
            "LR: 0.05\n",
            "Epoch: 0.34190956492008306\n",
            "Train Error: \n",
            " Accuracy: 78.1%, Loss: 0.845472\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.05\n",
            "Epoch: 0.5128643473801252\n",
            "Train Error: \n",
            " Accuracy: 78.1%, Loss: 0.744260\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.025\n",
            "Epoch: 0.6838191298401674\n",
            "Train Error: \n",
            " Accuracy: 68.8%, Loss: 0.814368\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.025\n",
            "Epoch: 0.8547739123002095\n",
            "Train Error: \n",
            " Accuracy: 75.0%, Loss: 0.702001\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0125\n",
            "Epoch: 1.0257286947602433\n",
            "Train Error: \n",
            " Accuracy: 87.5%, Loss: 0.645555\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0125\n",
            "Epoch: 1.19668347722023\n",
            "Train Error: \n",
            " Accuracy: 84.4%, Loss: 0.622416\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.00625\n",
            "Epoch: 1.3676382596802166\n",
            "Train Error: \n",
            " Accuracy: 90.6%, Loss: 0.399685\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.00625\n",
            "Epoch: 1.5385930421402032\n",
            "Train Error: \n",
            " Accuracy: 87.5%, Loss: 0.441229\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.003125\n",
            "Epoch: 1.7095478246001898\n",
            "Train Error: \n",
            " Accuracy: 81.2%, Loss: 0.498843\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.003125\n",
            "Epoch: 1.8805026070601765\n",
            "Train Error: \n",
            " Accuracy: 87.5%, Loss: 0.342036\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0015625\n",
            "Epoch: 2.0514573895201633\n",
            "Train Error: \n",
            " Accuracy: 84.4%, Loss: 0.448781\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0015625\n",
            "Epoch: 2.22241217198015\n",
            "Train Error: \n",
            " Accuracy: 78.1%, Loss: 0.633322\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.00078125\n",
            "Epoch: 2.3933669544401366\n",
            "Train Error: \n",
            " Accuracy: 84.4%, Loss: 0.541110\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.00078125\n",
            "Epoch: 2.564321736900123\n",
            "Train Error: \n",
            " Accuracy: 87.5%, Loss: 0.582443\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.000390625\n",
            "Epoch: 2.73527651936011\n",
            "Train Error: \n",
            " Accuracy: 71.9%, Loss: 0.879628\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.000390625\n",
            "Epoch: 2.9062313018200965\n",
            "Train Error: \n",
            " Accuracy: 81.2%, Loss: 0.804606\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0001953125\n",
            "Epoch: 3.077186084280083\n",
            "Train Error: \n",
            " Accuracy: 78.1%, Loss: 0.743375\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0001953125\n",
            "Epoch: 3.2481408667400697\n",
            "Train Error: \n",
            " Accuracy: 84.4%, Loss: 0.505497\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 9.765625e-05\n",
            "Epoch: 3.4190956492000564\n",
            "Train Error: \n",
            " Accuracy: 84.4%, Loss: 0.710287\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 9.765625e-05\n",
            "Test Error: \n",
            " Avg Accuracy: 72.5%, Avg Loss: 0.902346\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqf6_kLMUZUn"
      },
      "source": [
        "## Full Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2os_fvuNJmcJ"
      },
      "source": [
        "TODO:\n",
        "  - try other dataset\n",
        "  - compare with their implementation\n",
        "  - try train/test split across edges\n",
        "  - batch normalization\n",
        "  - add identity matrix to see performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_SC_jUkl-y"
      },
      "source": [
        "Train full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "UPx__jwgLUCL"
      },
      "outputs": [],
      "source": [
        "model = GAP(encoder, \n",
        "            PMA(K_hop, agg_sigma), \n",
        "            Classification(K_hop, [60, 20], [(K_hop+1)*20, num_classes]))\n",
        "# model = GAP(encoder, \n",
        "#             pmat, \n",
        "#             Classification(K_hop, [60, 20], [(K_hop+1)*20, num_classes]))\n",
        "model = model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "z22WhxLcaveo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d695cdcd-4f0c-459d-cd37-608670a0a6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Error: \n",
            " Accuracy: 93.8%, Loss: 0.390964\n",
            "Train Error: \n",
            " Accuracy: 93.8%, Loss: 0.130649\n",
            "Train Error: \n",
            " Accuracy: 96.9%, Loss: 0.260985\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Loss: 0.064982\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Loss: 0.000013\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Loss: 0.010245\n",
            "Train Error: \n",
            " Accuracy: 96.9%, Loss: 0.064983\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Loss: 0.000011\n",
            "Train Error: \n",
            " Accuracy: 100.0%, Loss: 0.040663\n",
            "Train Error: \n",
            " Accuracy: 96.9%, Loss: 0.064983\n",
            "Test Error: \n",
            " Avg Accuracy: 95.6%, Avg Loss: 0.202178\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "for t in range(10000):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, model, loss_fn, optimizer)\n",
        "    if (t + 1) % 1000 == 0:\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", model, loss_fn, True)\n",
        "    scheduler.step()\n",
        "test(test_loader, \"TEST\", model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgX93DaHqqzh"
      },
      "source": [
        "## Backup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # TEMP CODE\n",
        "# edge_index = torch.tensor([[0, 2, 0, 0, 2, 3, 1, 4, 1, 2, 4],\n",
        "#                            [1, 0, 3, 4, 1, 1, 4, 4, 1, 3, 3]], dtype=torch.long)\n",
        "# x = torch.tensor([[0, 1, 0], [1, 2, 2], [2, 3, 1], [3, 2, 4], [4, 2, 3]], dtype=torch.float)\n",
        "# y = torch.tensor([0, 1, 1, 2, 2], dtype=torch.long)\n",
        "# pma = PMAT(2, 3, 0)\n",
        "# AggregationModule.edge_index = edge_index\n",
        "# stack = pma(x)\n",
        "# print(stack)\n",
        "\n",
        "# # # plt.figure(figsize=(16,7))\n",
        "# # # plt.imshow(tensor)\n",
        "# # # plt.show()\n",
        "# #         # [encoder, pma, element_wise_mlp, combine, mlp]\n"
      ],
      "metadata": {
        "id": "-orS8HNbNcOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch_geometric.datasets import Amazon\n",
        "\n",
        "# dataset = Amazon('.', name='Computers')[0]\n",
        "# # prepare dataset by removing classes that have less than 1000 examples\n",
        "# dataset = prepare_dataset(dataset, 1000)\n",
        "# dataset = add_self_edges(dataset)\n",
        "# # get num classes\n",
        "# num_classes = torch.unique(dataset.y).size(dim=0)\n",
        "\n",
        "# # train/test split\n",
        "# train_dataset, test_dataset = train_test_split(dataset, 0.2)\n",
        "# print(train_dataset.edge_index.size(dim=1))\n",
        "# train_dataset = add_edge_to_low_degree_nodes(train_dataset, 10)\n",
        "# print(train_dataset.edge_index.size(dim=1))\n",
        "# test_dataset = add_edge_to_low_degree_nodes(test_dataset, 10)\n",
        "# train_dataset, test_dataset = standardization(train_dataset, test_dataset)"
      ],
      "metadata": {
        "id": "JBYlKXUONVyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vof5FsrKqroZ"
      },
      "outputs": [],
      "source": [
        "# # train\n",
        "# def train(dataloader, model, loss_fn, optimizer, print_every = 100):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     model.train()\n",
        "#     for batch, (X, y) in enumerate(dataloader):\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "\n",
        "#         # Compute prediction error\n",
        "#         pred = model(X)\n",
        "#         loss = loss_fn(pred, y)\n",
        "\n",
        "#         # Backpropagation\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if batch % print_every == 0:\n",
        "#             loss, current = loss.item(), batch * len(X)\n",
        "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# # test\n",
        "# def test(dataloader, model, loss_fn):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     num_batches = len(dataloader)\n",
        "#     model.eval()\n",
        "#     test_loss, correct = 0, 0\n",
        "#     with torch.inference_mode():\n",
        "#         for X, y in dataloader:\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             pred = model(X)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= num_batches\n",
        "#     correct /= size\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch_geometric.loader import NeighborLoader\n",
        "# from torch_geometric.data import Data\n",
        "# edge_index = torch.tensor([[0, 2, 0, 0, 2, 3, 1, 4, 1, 2, 4],\n",
        "#                            [1, 0, 3, 4, 1, 1, 4, 4, 1, 3, 3]], dtype=torch.long)\n",
        "# x = torch.tensor([[0, 1, 0], [1, 2, 2], [2, 3, 1], [3, 2, 4], [4, 2, 3]], dtype=torch.float)\n",
        "# y = torch.tensor([0, 1, 1, 2, 2], dtype=torch.long)\n",
        "# data = Data(x=x, y=y, edge_index=edge_index)\n",
        "# loader = NeighborLoader(data, [10], batch_size=2, shuffle=True)\n",
        "# batch = next(iter(loader))\n",
        "# print(batch.x)\n",
        "# print(batch.edge_index)\n",
        "# print(batch.x[:batch.batch_size, ])\n",
        "# data = add_edge_to_low_degree_nodes(data, 3)\n",
        "# print(data.x)\n",
        "# data = standardization(data)\n",
        "# print(data.x)\n",
        "# print(data.x.std(dim=0))\n",
        "# print(data.edge_index)\n",
        "# print(get_adjacency_matrix(data.edge_index, x.size(dim=0)).to_dense())\n",
        "# train, test = train_test_split(data, 0.2)\n",
        "# print(train.x)\n",
        "# print(train.edge_index)\n",
        "# print(test.x)\n",
        "# print(test.edge_index)"
      ],
      "metadata": {
        "id": "WAnNgCsXNMx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5064436-270d-4907-e2d2-063dc9144e32"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 2., 3.],\n",
            "        [3., 2., 4.],\n",
            "        [0., 1., 0.],\n",
            "        [1., 2., 2.],\n",
            "        [2., 3., 1.]])\n",
            "tensor([[2, 3, 0, 2, 4, 0],\n",
            "        [0, 0, 0, 1, 1, 1]])\n",
            "tensor([[4., 2., 3.],\n",
            "        [3., 2., 4.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Sigma calculated above in node-level and edge-level DP case\n",
        "# gap = GAP(encoder, PMA(A, K_hop, sigma), Classification(K_hop, [60, 30, 20], [(K_hop+1)*20, 60, 30, num_classes]))\n",
        "# gap_model = gap.to(device)\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(gap_model.parameters(), lr=1e-1)\n",
        "\n",
        "# # if node_level:\n",
        "# #   optimizer = op.optimizers.optimizer.DPOptimizer(\n",
        "# #       # TODO: Fill out these parameters '?'\n",
        "# #       optimizer=optimizer,\n",
        "# #       noise_multiplier=?,\n",
        "# #       max_grad_norm=?,\n",
        "# #       loss_reduction='sum'\n",
        "# #   ) \n",
        "\n",
        "# epochs = 500\n",
        "# for t in range(epochs):\n",
        "#     # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "#     train(X, y, gap_model, loss_fn, optimizer)\n",
        "#     if t % 10 == 0:\n",
        "#       test(X, y, gap_model, loss_fn)\n",
        "# print(\"Done!\")"
      ],
      "metadata": {
        "id": "wrf0EaGHNQW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vb1hVZklFoDP",
        "GF264FWv0pJI",
        "Ty_dao27oEAB",
        "WebCohAOIX0P",
        "XDJP3FhOi_Ty",
        "E7oEXJ63oyl-",
        "a4Md2Ds3q3CY",
        "WOoJSnrTSVWZ",
        "BgX93DaHqqzh"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}