{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lua-Nova/Modern-GAP-GNN/blob/new/ModernGAP_Experiment_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aFQCNsDWU5mf",
        "outputId": "ea5bfad6-adf4-496c-aa02-42f66bd69d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.1.0%2Bpt112cu113-cp38-cp38-linux_x86_64.whl (8.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9 MB 6.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt112cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15%2Bpt112cu113-cp38-cp38-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15+pt112cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0%2Bpt112cu113-cp38-cp38-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-cluster) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-cluster) (1.21.6)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0+pt112cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-gxhs6sji\n",
            "  Running command git clone -q https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-gxhs6sji\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.2.0) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.2.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.2.0) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.2.0) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.2.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.2.0) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric==2.2.0) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 34.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric==2.2.0) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric==2.2.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric==2.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric==2.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric==2.2.0) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric==2.2.0) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric==2.2.0) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773337 sha256=9e85bf162396ae35fe97f8bc45933e1e9138efb185a6043b0cea8f1fe7c77c89\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c32yvdfb/wheels/ba/e1/8e/28297c3201c884d3ea8c47ba71a9e71e547e556c0caa9cf5a2\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping pyvacy as it is not installed.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvacy\n",
            "  Downloading pyvacy-0.0.32.tar.gz (10 kB)\n",
            "Building wheels for collected packages: pyvacy\n",
            "  Building wheel for pyvacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyvacy: filename=pyvacy-0.0.32-py3-none-any.whl size=12401 sha256=a15a1a5047c78af0538020cd0c5e80f35a768f5c4bee32da41e674fe9bfebe1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/b9/fd/45fb9afde4d0efd0022f976cf3e53f058e01cf99812687e36f\n",
            "Successfully built pyvacy\n",
            "Installing collected packages: pyvacy\n",
            "Successfully installed pyvacy-0.0.32\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  #NVIDIA GPU version\n",
        "  %pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "  %pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "  %pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "  %pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "  %pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "else:\n",
        "  #CPU version\n",
        "  %pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric==2.0.0 -f https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
        "%pip uninstall pyvacy  --y\n",
        "%pip install pyvacy\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBxqGGgYU6k-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import Sequential, GCNConv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb1hVZklFoDP"
      },
      "source": [
        "## Encoder Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ScntAVtWL3M"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(11)\n",
        "# create classes for layers that are used a lot to avoid repeating code\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  # e.g. dimensions = [50,40,30,20]\n",
        "    def __init__(self, dimensions):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        layers = []\n",
        "        for i in range(len(dimensions)-1):\n",
        "          layers.append(nn.Linear(dimensions[i], dimensions[i+1]))\n",
        "          layers.append(nn.SELU(inplace=True))\n",
        "\n",
        "        self.linear_selu_stack = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_selu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF264FWv0pJI"
      },
      "source": [
        "## PMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMzLWqwF03Uu"
      },
      "outputs": [],
      "source": [
        "class AggregationModule(nn.Module):\n",
        "  edge_index = None\n",
        "\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "class PMA(AggregationModule):\n",
        "    # A - adjacency matrix     TODO: this should not be given to the module itself, it should access it in training (or from the graph dataset)\n",
        "    # num_hops - the number of hops covered by this GNN\n",
        "    def __init__(self, num_hops, sigma):\n",
        "        super().__init__()\n",
        "        self.num_hops = num_hops\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TEMP SOLUTION\n",
        "        if AggregationModule.edge_index is None:\n",
        "          raise RuntimeError(\"Set AggregationModule.edge_index [TEMP SOLUTION] before running\")\n",
        "        edge_index = AggregationModule.edge_index\n",
        "        A = get_adjacency_matrix(edge_index, x.size(dim=0))\n",
        "        out = [torch.nn.functional.normalize(x, dim=1)]\n",
        "        for k in range(self.num_hops):\n",
        "            aggr = torch.mm(torch.transpose(A, 0, 1), out[-1])\n",
        "            noised = aggr + torch.normal(torch.zeros(aggr.size()), std=self.sigma).to(device)\n",
        "            normalized = torch.nn.functional.normalize(noised, dim=1)\n",
        "            out.append(normalized)\n",
        "        return torch.stack(out)\n",
        "        # return torch.nn.functional.normalize(x, dim=1)\n",
        "\n",
        "class PMAT(AggregationModule):\n",
        "    # def __init__(self, num_hops, transform_dimensions):\n",
        "    def __init__(self, num_hops, encoding_dimensions, sigma):\n",
        "        super().__init__()\n",
        "        self.num_hops = num_hops\n",
        "        self.sigma = sigma\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # self.transforms = nn.ModuleList()\n",
        "        self.attentions = nn.ModuleList()\n",
        "        for i in range(num_hops):\n",
        "          # self.transforms.append(nn.Linear(*transform_dimensions)) # Only 1 layer transformation\n",
        "          # self.attentions.append(MLP([2*transform_dimensions[-1], 1])) # Attention mechanism takes 2 encodings and outputs 1 weight\n",
        "          # TODO: Figure out if we want a transformer?\n",
        "          self.attentions.append(MLP([2*encoding_dimensions, 1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TEMP SOLUTION\n",
        "        if AggregationModule.edge_index is None:\n",
        "          raise RuntimeError(\"Set AggregationModule.edge_index [TEMP SOLUTION] before running\")\n",
        "        edge_index = AggregationModule.edge_index\n",
        "        out = [torch.nn.functional.normalize(x, dim=1)]\n",
        "        for k in range(self.num_hops):\n",
        "            # Do we need to do a transform? I reckon we can use raw encoding and aggregate according to attention (and then the Classification module)\n",
        "            # can handle how the aggregations get transformed\n",
        "            # h = self.transforms[k](out[-1])\n",
        "            h = out[-1]\n",
        "            e_values = self.attentions[k](h[edge_index.T].reshape(edge_index.size(dim=1), 2*h.size(dim=1))) # DPSGD to guarantee DP attention training\n",
        "            # we have to use Sigmoid because if we use Softmax, removing an edge will change the weight of all other edges in the neighbourhood\n",
        "            alpha_values = self.sigmoid(e_values)\n",
        "            alpha = torch.sparse_coo_tensor(edge_index,\n",
        "                                            alpha_values.reshape(edge_index.size(dim=1)),\n",
        "                                            (x.size(dim=0), x.size(dim=0)),\n",
        "                                            dtype=torch.float).transpose(0, 1)\n",
        "\n",
        "            aggr = torch.sparse.mm(alpha, h)\n",
        "            # Might need to not use \"transforms\" and instead do raw aggregations like the original PMA\n",
        "            # aggr = torch.mm(self.A, out[-1]) \n",
        "            # noised = aggr # TODO: add noise # Gaussian mechanism to guarantee DP for neighbourhood aggregation\n",
        "            noised = aggr + torch.normal(torch.zeros(aggr.size()), std=self.sigma).to(device)\n",
        "            normalized = torch.nn.functional.normalize(noised, dim=1)\n",
        "            out.append(normalized)\n",
        "        return torch.stack(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty_dao27oEAB"
      },
      "source": [
        "## Classification Module\n",
        "NOTE: \n",
        "\n",
        "MLP base: The first MLP in the cassification module. \n",
        "\n",
        "MLP head: The last MLP and takes the combined output of all MLP base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dLm3Q3HofT6"
      },
      "outputs": [],
      "source": [
        "class Classification(nn.Module):\n",
        "    # num_hops - the number of hops covered by this GNN\n",
        "    # encoder_dimensions - the MLP dimensions of each base MLP\n",
        "    # head_dimensions - the dimensions of the head MLP\n",
        "    def __init__(self, num_hops, encoder_dimensions, head_dimensions):\n",
        "        super().__init__()\n",
        "        self.base_mlps = nn.ModuleList()\n",
        "        self.num_hops = num_hops\n",
        "        if encoder_dimensions:\n",
        "          for i in range(num_hops+1):\n",
        "              self.base_mlps.append(MLP(encoder_dimensions))\n",
        "        self.head_mlp = MLP(head_dimensions) # TODO: should this be softmax? I think we add a softmax for classification tasks. We can test if it works better\n",
        "    \n",
        "    def forward(self, cache):\n",
        "        # forward through bases\n",
        "        out = []\n",
        "        for i in range(self.num_hops+1):\n",
        "          if self.base_mlps:\n",
        "            encoding = self.base_mlps[i](cache[i,:,:])\n",
        "            out.append(encoding) # add corresponding encoding\n",
        "          else:\n",
        "            out.append(cache[i, :, :])\n",
        "        # combine (use concatenation)\n",
        "        combined_x = torch.cat(out, dim=1)\n",
        "        # forward through head\n",
        "        return self.head_mlp(combined_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evGOLmy-UrMs"
      },
      "outputs": [],
      "source": [
        "class GAP(nn.Module):\n",
        "  # encoder - pretrained encoder module\n",
        "  # pma - PMA module\n",
        "  # classification - classification module\n",
        "  def __init__(self, encoder, pma, classification): # TODO: decide whether we should recieve the models as parameters\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.encoder.requires_grad=False\n",
        "    self.pma = pma\n",
        "    self.classification = classification\n",
        "\n",
        "  def forward(self, x):\n",
        "    # initial node encoding\n",
        "    x_encoded = self.encoder(x)\n",
        "    # aggregation module\n",
        "    cache = self.pma(x_encoded) \n",
        "    # classification\n",
        "    return self.classification(cache) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WebCohAOIX0P"
      },
      "source": [
        "##Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_Pf4P_bIZ5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb86647-d17b-432f-acef-5e6a84c88ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epsilon: 1.00, Delta: 0.50, Sigma: 1.51\n"
          ]
        }
      ],
      "source": [
        "node_level = True\n",
        "\n",
        "# Edge level DP\n",
        "agg_epsilon, agg_delta, epsilon_1, epsilon_5, pmat_epsilon, pmat_delta = 1, 0.5, 4, 4, 4, 0.5\n",
        "K_hop = 1\n",
        "agg_sigma = 1 / np.max(np.roots([K_hop/2, np.sqrt(2*K_hop*np.log(1/agg_delta)), -agg_epsilon]))\n",
        "# Node level DP\n",
        "if (node_level):\n",
        "  pass\n",
        "  # How do we calculate this?\n",
        "data = \"reddit\"\n",
        "batch_size = 32\n",
        "\n",
        "print(f\"Epsilon: {agg_epsilon:>0.2f}, Delta: {agg_delta:>0.2f}, Sigma: {agg_sigma:>0.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "XDJP3FhOi_Ty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPX-rb88ukrc"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "# this method partitions based on nodes (so edges between splits are not used)\n",
        "def train_test_split(dataset, test_ratio):\n",
        "    X, y, edge_index= dataset.x, dataset.y, dataset.edge_index\n",
        "    shuffle_ordering = torch.randperm(X.size(dim=0))\n",
        "\n",
        "    edge_mapping = torch.zeros(X.size(dim=0), dtype=torch.long)\n",
        "    edge_mapping[shuffle_ordering] = torch.arange(X.size(dim=0))\n",
        "\n",
        "    X = X[shuffle_ordering]\n",
        "    y = y[shuffle_ordering]\n",
        "    edge_index = edge_mapping[edge_index]\n",
        "\n",
        "    mask = torch.zeros(X.size(dim=0), dtype=torch.bool)\n",
        "    train_slice = int((1-test_ratio)*X.size(dim=0))\n",
        "    mask[:train_slice] = True\n",
        "\n",
        "    X_train = X[mask]\n",
        "    X_test = X[~mask]\n",
        "\n",
        "    y_train = y[mask]\n",
        "    y_test = y[~mask]\n",
        "\n",
        "    edge_index_train = edge_index[:, torch.logical_and(*mask[edge_index])]\n",
        "    edge_index_test = edge_index[:, torch.logical_and(*~mask[edge_index])] - train_slice\n",
        "\n",
        "    return Data(x=X_train, y=y_train, edge_index=edge_index_train), \\\n",
        "           Data(x=X_test, y=y_test, edge_index=edge_index_test)\n",
        "\n",
        "\n",
        "# returns filtered edge index, first removes edges that have removed src or dst nodes, then shifts indices of remained src/dst nodes\n",
        "def filter_edge_index(edge_index, filter):\n",
        "\n",
        "    node_indices = torch.arange(filter.size(dim=0))[filter]\n",
        "    edge_mapping = torch.zeros(filter.size(dim=0), dtype=torch.long)\n",
        "    edge_mapping[node_indices] = torch.arange(node_indices.size(dim=0))\n",
        "\n",
        "\n",
        "    edge_index = edge_index.to(torch.long)\n",
        "    edge_filter = torch.logical_and(*filter[edge_index])\n",
        "    return edge_mapping[edge_index[:, edge_filter]]\n",
        "\n",
        "def add_edge_to_low_degree_nodes(dataset, low_degree_threshold):\n",
        "    X, y, edge_index = dataset.x, dataset.y, dataset.edge_index\n",
        "\n",
        "    # get low degree nodes\n",
        "    A = get_adjacency_matrix(edge_index, X.size(dim=0))\n",
        "    sums = torch.sparse.sum(A, dim=1).to_dense()\n",
        "    mask = sums < low_degree_threshold\n",
        "    \n",
        "    # get edge_index mask for neighbours of low degree nodes\n",
        "    filter = mask[edge_index[0, :]]\n",
        "    low_degree_edges_index = edge_index[:, filter]\n",
        "    low_degree_A = get_adjacency_matrix(low_degree_edges_index, X.size(dim=0))\n",
        "\n",
        "    # get 1-hop neighbours and add to A \n",
        "    # NOTE: without sampling (just adds all 1-hop neighbours)\n",
        "    one_hop_low_degree_A = torch.sparse.mm(low_degree_A, A)\n",
        "    new_edge_index = torch.add(one_hop_low_degree_A, A).coalesce().indices()\n",
        "    return Data(x=X, y=y, edge_index=new_edge_index)\n",
        "    \n",
        "\n",
        "def prepare_dataset(dataset, threshold):\n",
        "    X, y, edge_index = dataset.x, dataset.y, dataset.edge_index\n",
        "\n",
        "    # remove labels with less examples than threshold\n",
        "    index_map = torch.zeros(y.size())\n",
        "    included_classes = y.unique(return_counts=True)[1] >= threshold\n",
        "    filter = included_classes[y]\n",
        "    # remap labels (i.e. if they were 0-8 and we remove 4 labels, new labels should be between 0 and 4)\n",
        "    label_mapping = torch.zeros(included_classes.size(dim=0), dtype=torch.long)\n",
        "    label_mapping[included_classes] = torch.arange(torch.count_nonzero(included_classes))\n",
        "\n",
        "    y = label_mapping[y[filter]].to(torch.long)\n",
        "    X = X[filter]\n",
        "\n",
        "    # remove edges that had their nodes removed\n",
        "    edge_index = filter_edge_index(edge_index, filter)\n",
        "\n",
        "    return Data(x=X, y=y, edge_index=edge_index)\n",
        "\n",
        "# make sparse adjacency matrix, A\n",
        "def get_adjacency_matrix(edge_index, num_nodes):\n",
        "    values = torch.ones(edge_index.size(dim=1), dtype = torch.int)\n",
        "    A = torch.sparse_coo_tensor(edge_index, values, (num_nodes, num_nodes), dtype=torch.float)\n",
        "    return A\n",
        "\n",
        "def standardization(train_dataset, test_dataset):\n",
        "    X = train_dataset.x\n",
        "    means = X.mean(dim=0, keepdim=True)\n",
        "    stds = X.std(dim=0, keepdim=True)\n",
        "    X_train = (X - means) / stds\n",
        "    X_test = (test_dataset.x - means) / stds\n",
        "    return Data(x=X_train, y=train_dataset.y, edge_index=train_dataset.edge_index), Data(x=X_test, y=test_dataset.y, edge_index=test_dataset.edge_index)\n",
        "\n",
        "def add_self_edges(dataset):\n",
        "    X = dataset.x\n",
        "    self_edges = torch.stack((torch.arange(X.size(dim=0)), torch.arange(X.size(dim=0))))\n",
        "    edge_index = torch.cat((dataset.edge_index, self_edges), dim=1)\n",
        "    return Data(x=X, y=dataset.y, edge_index=edge_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7oEXJ63oyl-"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Reddit\n",
        "dataset = Reddit('.')[0]\n",
        "# prepare dataset by removing classes that have less than 1000 examples\n",
        "dataset = prepare_dataset(dataset, 10000)\n",
        "dataset = add_self_edges(dataset)\n",
        "# get num classes\n",
        "num_classes = torch.unique(dataset.y).size(dim=0)\n",
        "\n",
        "# train/test split\n",
        "train_dataset, test_dataset = train_test_split(dataset, 0.2)\n",
        "print(train_dataset.edge_index.size(dim=1))\n",
        "# train_dataset = add_edge_to_low_degree_nodes(train_dataset, 10)\n",
        "print(train_dataset.edge_index.size(dim=1))\n",
        "# test_dataset = add_edge_to_low_degree_nodes(test_dataset, 10)\n",
        "train_dataset, test_dataset = standardization(train_dataset, test_dataset)"
      ],
      "metadata": {
        "id": "JrAKSDBl03mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6248d301-87e0-49d2-a36d-095c9d45af5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.dgl.ai/dataset/reddit.zip\n",
            "Extracting ./raw/reddit.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29495772\n",
            "29495772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29Swawd9we8f"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "X_train, y_train, edge_index_train = train_dataset.x, train_dataset.y, train_dataset.edge_index\n",
        "X_test, y_test, edge_index_test = test_dataset.x, test_dataset.y, test_dataset.edge_index\n",
        "\n",
        "# using large number like 10,000 so that all neighbours are sampled \n",
        "# I don't like how it samples, so I'm just gonna sample everything\n",
        "train_loader = NeighborLoader(train_dataset, num_neighbors=[X_train.size(dim=0)]*K_hop, \n",
        "                              batch_size=batch_size, shuffle=True)\n",
        "test_loader = NeighborLoader(test_dataset, num_neighbors=[X_test.size(dim=0)]*K_hop, \n",
        "                             batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Md2Ds3q3CY"
      },
      "source": [
        "## Train/Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu0AtfY7qzba"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "def train(batch, model, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  X, y = batch.x.to(device), batch.y.to(device)\n",
        "  AggregationModule.edge_index = batch.edge_index.to(device)\n",
        "  # compute prediction error\n",
        "  pred = model(X)\n",
        "  loss = loss_fn(pred[batch.train_mask], y[batch.train_mask])\n",
        "  # backpropagation\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# test batch\n",
        "def batch_test(batch, split, model, loss_fn, wordy=False):\n",
        "    size = batch.x.size(dim=0)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.inference_mode():\n",
        "        X, y = batch.x.to(device), batch.y.to(device)\n",
        "        AggregationModule.edge_index = batch.edge_index.to(device)\n",
        "        pred = model(X)\n",
        "        test_loss = loss_fn(pred[batch.train_mask], y[batch.train_mask]).item()\n",
        "        correct = (pred.argmax(1) == y).type(torch.float).sum().item() / size\n",
        "    if wordy:\n",
        "      print(f\"{split.title()} Error: \\n Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f}\")\n",
        "    return test_loss, correct\n",
        "\n",
        "# test\n",
        "def test(loader, split, model, loss_fn):\n",
        "    size = len(loader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for batch in loader:\n",
        "        batch_loss, batch_correct = batch_test(batch, split, model, loss_fn)\n",
        "        test_loss += batch_loss\n",
        "        correct += batch_correct\n",
        "    correct /= size\n",
        "    test_loss /= size\n",
        "    print(f\"{split.title()} Error: \\n Avg Accuracy: {(100*correct):>0.1f}%, Avg Loss: {test_loss:>8f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFvQEYa3j3O7"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XixvkAQpqcFM"
      },
      "source": [
        "Encoder Design\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZT6RQbkiTbj"
      },
      "outputs": [],
      "source": [
        "# encoder\n",
        "dimensions = [602, 300, 60]\n",
        "encoder_model = nn.Sequential(\n",
        "    MLP(dimensions),\n",
        "    nn.Linear(dimensions[-1], num_classes),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Jk2-tbqLer"
      },
      "source": [
        "Encoder Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HYGNjZgqo7k",
        "outputId": "3a6acb22-8874-4fa6-d6c5-b557bf9209f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Error: \n",
            " Accuracy: 58.5%, Loss: 1.785143\n",
            "Train Error: \n",
            " Accuracy: 71.3%, Loss: 1.598296\n",
            "Train Error: \n",
            " Accuracy: 78.6%, Loss: 1.506701\n",
            "Train Error: \n",
            " Accuracy: 81.1%, Loss: 1.476494\n",
            "Train Error: \n",
            " Accuracy: 76.4%, Loss: 1.518702\n",
            "Train Error: \n",
            " Accuracy: 81.3%, Loss: 1.471633\n",
            "Train Error: \n",
            " Accuracy: 76.2%, Loss: 1.521953\n",
            "Train Error: \n",
            " Accuracy: 80.0%, Loss: 1.481204\n",
            "Train Error: \n",
            " Accuracy: 80.8%, Loss: 1.474878\n",
            "Train Error: \n",
            " Accuracy: 82.1%, Loss: 1.459544\n",
            "Test Error: \n",
            " Avg Accuracy: 79.4%, Avg Loss: 1.484350\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "encoder_model = encoder_model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(encoder_model.parameters(), lr=1e-3)\n",
        "\n",
        "# if node_level:\n",
        "#   optimizer = op.optimizers.optimizer.DPOptimizer(\n",
        "#       # TODO: Fill out these parameters '?'\n",
        "#       optimizer=optimizer,\n",
        "#       noise_multiplier=?,\n",
        "#       max_grad_norm=?\n",
        "#   )\n",
        "\n",
        "for t in range(100):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, encoder_model, loss_fn, optimizer)\n",
        "    if (t + 1) % 10 == 0:\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", encoder_model, loss_fn, True)\n",
        "test(test_loader, \"TEST\", encoder_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "encoder = encoder_model[0]\n",
        "encoder.requires_grad=False\n",
        "\n",
        "# for name, param in encoder_model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name, param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PMAT Training"
      ],
      "metadata": {
        "id": "WOoJSnrTSVWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvacy import optim, analysis\n",
        "# PMAT\n",
        "pmat_model = nn.Sequential(\n",
        "    encoder,\n",
        "    PMAT(K_hop, 60, agg_sigma),\n",
        "    Classification(K_hop, [], [(K_hop+1)*60, num_classes])\n",
        ")\n",
        "pmat_model = pmat_model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(encoder_model.parameters(), lr=1e-3)\n",
        "# TODO: Not allowed dyanmic batch_size for DPSGD, our batches are edge-wise\n",
        "# so they should have fixed batch_size!\n",
        "optimizer = optim.DPAdam(\n",
        "    l2_norm_clip=1.0,\n",
        "    noise_multiplier=1.0,\n",
        "    batch_size=batch_size,\n",
        "    params=pmat_model.parameters(),\n",
        "    lr=0.5e-1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)"
      ],
      "metadata": {
        "id": "LKxUUyl_SW-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_epochs = 0\n",
        "for t in range(10000):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, pmat_model, loss_fn, optimizer)\n",
        "    edge_epochs += batch_size / X_train.size(dim=0)\n",
        "    epsilon = analysis.moments_accountant(X_train.size(dim=0), batch_size, \n",
        "                                          1.0, edge_epochs, pmat_delta)\n",
        "    if (t + 1) % 500 == 0:\n",
        "      print(\"Epoch:\", edge_epochs)\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", pmat_model, loss_fn, True)\n",
        "      print(\"Optimizer Achieves ({:>0.1f}, {})-DP\".format(epsilon, pmat_delta))\n",
        "      print(\"LR:\", scheduler.get_last_lr()[0])\n",
        "    scheduler.step()\n",
        "    if epsilon >= pmat_epsilon:\n",
        "      break\n",
        "test(test_loader, \"TEST\", pmat_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "pmat = pmat_model[1]\n",
        "pmat.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CpsiubMeSoTE",
        "outputId": "79f7d0c9-c479-4837-9a59-c2cbcf31ec6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0.1725186918314935\n",
            "Train Error: \n",
            " Accuracy: 29.6%, Loss: 2.006937\n",
            "Optimizer Achieves (0.0, 0.5)-DP\n",
            "LR: 0.05\n",
            "Epoch: 0.343305406618955\n",
            "Train Error: \n",
            " Accuracy: 36.8%, Loss: 1.827637\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.05\n",
            "Epoch: 0.5154102764287708\n",
            "Train Error: \n",
            " Accuracy: 33.0%, Loss: 1.926981\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.025\n",
            "Epoch: 0.6859178664657434\n",
            "Train Error: \n",
            " Accuracy: 27.6%, Loss: 2.036926\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.025\n",
            "Epoch: 0.8540284010874503\n",
            "Train Error: \n",
            " Accuracy: 38.0%, Loss: 1.815643\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0125\n",
            "Epoch: 1.0228958916552504\n",
            "Train Error: \n",
            " Accuracy: 40.8%, Loss: 1.763620\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0125\n",
            "Epoch: 1.1935167182605022\n",
            "Train Error: \n",
            " Accuracy: 45.6%, Loss: 1.692450\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.00625\n",
            "Epoch: 1.3657938161442267\n",
            "Train Error: \n",
            " Accuracy: 41.1%, Loss: 1.747749\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.00625\n",
            "Epoch: 1.5341317053847605\n",
            "Train Error: \n",
            " Accuracy: 34.3%, Loss: 1.854498\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.003125\n",
            "Epoch: 1.7018665929476287\n",
            "Train Error: \n",
            " Accuracy: 33.0%, Loss: 1.870617\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.003125\n",
            "Epoch: 1.871184453148067\n",
            "Train Error: \n",
            " Accuracy: 38.6%, Loss: 1.790308\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0015625\n",
            "Epoch: 2.04165539386459\n",
            "Train Error: \n",
            " Accuracy: 31.7%, Loss: 1.931765\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.0015625\n",
            "Epoch: 2.214934465861756\n",
            "Train Error: \n",
            " Accuracy: 52.4%, Loss: 1.522395\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.00078125\n",
            "Epoch: 2.386567844367662\n",
            "Train Error: \n",
            " Accuracy: 59.3%, Loss: 1.409414\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.00078125\n",
            "Epoch: 2.55806428799355\n",
            "Train Error: \n",
            " Accuracy: 27.0%, Loss: 2.003076\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.000390625\n",
            "Epoch: 2.728974274685878\n",
            "Train Error: \n",
            " Accuracy: 42.5%, Loss: 1.737823\n",
            "Optimizer Achieves (0.1, 0.5)-DP\n",
            "LR: 0.000390625\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-6ed7ec9ce84a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmat_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0medge_epochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0medge_index_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mavg_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0medge_epochs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0medge_index_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1403fb668251>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqf6_kLMUZUn"
      },
      "source": [
        "## Full Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2os_fvuNJmcJ"
      },
      "source": [
        "TODO:\n",
        "  - try other dataset\n",
        "  - compare with their implementation\n",
        "  - try train/test split across edges\n",
        "  - batch normalization\n",
        "  - add identity matrix to see performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_SC_jUkl-y"
      },
      "source": [
        "Train full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPx__jwgLUCL"
      },
      "outputs": [],
      "source": [
        "model = GAP(encoder, \n",
        "            PMA(K_hop, agg_sigma), \n",
        "            Classification(K_hop, [60, 20], [(K_hop+1)*20, num_classes]))\n",
        "# model = GAP(encoder, \n",
        "#             PMAT(K_hop, 60, agg_sigma), \n",
        "#             Classification(K_hop, [60, 20], [(K_hop+1)*20, num_classes]))\n",
        "model = model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z22WhxLcaveo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a207048-a8e5-4cf9-f27f-8882c81fe072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Error: \n",
            " Accuracy: 50.0%, Loss: 1.086587\n",
            "Train Error: \n",
            " Accuracy: 70.8%, Loss: 0.908764\n",
            "Train Error: \n",
            " Accuracy: 81.5%, Loss: 0.759028\n",
            "Train Error: \n",
            " Accuracy: 85.6%, Loss: 0.639309\n",
            "Train Error: \n",
            " Accuracy: 88.4%, Loss: 0.542409\n",
            "Train Error: \n",
            " Accuracy: 89.9%, Loss: 0.495274\n",
            "Train Error: \n",
            " Accuracy: 91.3%, Loss: 0.453974\n",
            "Train Error: \n",
            " Accuracy: 91.9%, Loss: 0.418225\n",
            "Train Error: \n",
            " Accuracy: 92.4%, Loss: 0.387867\n",
            "Train Error: \n",
            " Accuracy: 92.7%, Loss: 0.362565\n",
            "Train Error: \n",
            " Accuracy: 93.0%, Loss: 0.350013\n",
            "Train Error: \n",
            " Accuracy: 93.1%, Loss: 0.337904\n",
            "Train Error: \n",
            " Accuracy: 93.4%, Loss: 0.326529\n",
            "Train Error: \n",
            " Accuracy: 93.5%, Loss: 0.316240\n",
            "Train Error: \n",
            " Accuracy: 93.5%, Loss: 0.306924\n",
            "Train Error: \n",
            " Accuracy: 93.5%, Loss: 0.301947\n",
            "Train Error: \n",
            " Accuracy: 93.7%, Loss: 0.297793\n",
            "Train Error: \n",
            " Accuracy: 93.8%, Loss: 0.293428\n",
            "Train Error: \n",
            " Accuracy: 93.7%, Loss: 0.288749\n",
            "Train Error: \n",
            " Accuracy: 93.8%, Loss: 0.284526\n",
            "Test Error: \n",
            " Accuracy: 90.0%, Loss: 0.391147\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "for t in range(100):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, model, loss_fn, optimizer)\n",
        "    if (t + 1) % 10 == 0:\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", model, loss_fn, True)\n",
        "    scheduler.step()\n",
        "test(test_loader, \"TEST\", model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgX93DaHqqzh"
      },
      "source": [
        "## Backup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # TEMP CODE\n",
        "# edge_index = torch.tensor([[0, 2, 0, 0, 2, 3, 1, 4, 1, 2, 4],\n",
        "#                            [1, 0, 3, 4, 1, 1, 4, 4, 1, 3, 3]], dtype=torch.long)\n",
        "# x = torch.tensor([[0, 1, 0], [1, 2, 2], [2, 3, 1], [3, 2, 4], [4, 2, 3]], dtype=torch.float)\n",
        "# y = torch.tensor([0, 1, 1, 2, 2], dtype=torch.long)\n",
        "# pma = PMAT(2, 3, 0)\n",
        "# AggregationModule.edge_index = edge_index\n",
        "# stack = pma(x)\n",
        "# print(stack)\n",
        "\n",
        "# # # plt.figure(figsize=(16,7))\n",
        "# # # plt.imshow(tensor)\n",
        "# # # plt.show()\n",
        "# #         # [encoder, pma, element_wise_mlp, combine, mlp]\n"
      ],
      "metadata": {
        "id": "-orS8HNbNcOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch_geometric.datasets import Amazon\n",
        "\n",
        "# dataset = Amazon('.', name='Computers')[0]\n",
        "# # prepare dataset by removing classes that have less than 1000 examples\n",
        "# dataset = prepare_dataset(dataset, 1000)\n",
        "# dataset = add_self_edges(dataset)\n",
        "# # get num classes\n",
        "# num_classes = torch.unique(dataset.y).size(dim=0)\n",
        "\n",
        "# # train/test split\n",
        "# train_dataset, test_dataset = train_test_split(dataset, 0.2)\n",
        "# print(train_dataset.edge_index.size(dim=1))\n",
        "# train_dataset = add_edge_to_low_degree_nodes(train_dataset, 10)\n",
        "# print(train_dataset.edge_index.size(dim=1))\n",
        "# test_dataset = add_edge_to_low_degree_nodes(test_dataset, 10)\n",
        "# train_dataset, test_dataset = standardization(train_dataset, test_dataset)"
      ],
      "metadata": {
        "id": "JBYlKXUONVyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vof5FsrKqroZ"
      },
      "outputs": [],
      "source": [
        "# # train\n",
        "# def train(dataloader, model, loss_fn, optimizer, print_every = 100):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     model.train()\n",
        "#     for batch, (X, y) in enumerate(dataloader):\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "\n",
        "#         # Compute prediction error\n",
        "#         pred = model(X)\n",
        "#         loss = loss_fn(pred, y)\n",
        "\n",
        "#         # Backpropagation\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if batch % print_every == 0:\n",
        "#             loss, current = loss.item(), batch * len(X)\n",
        "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# # test\n",
        "# def test(dataloader, model, loss_fn):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     num_batches = len(dataloader)\n",
        "#     model.eval()\n",
        "#     test_loss, correct = 0, 0\n",
        "#     with torch.inference_mode():\n",
        "#         for X, y in dataloader:\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             pred = model(X)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= num_batches\n",
        "#     correct /= size\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# edge_index = torch.tensor([[0, 2, 0, 0, 2, 3, 1, 4, 1, 2, 4],\n",
        "#                            [1, 0, 3, 4, 1, 1, 4, 4, 1, 3, 3]], dtype=torch.long)\n",
        "# x = torch.tensor([[0, 1, 0], [1, 2, 2], [2, 3, 1], [3, 2, 4], [4, 2, 3]], dtype=torch.float)\n",
        "# y = torch.tensor([0, 1, 1, 2, 2], dtype=torch.long)\n",
        "# data = Data(x=x, y=y, edge_index=edge_index)\n",
        "# add_self_edges(data)\n",
        "# data = add_edge_to_low_degree_nodes(data, 3)\n",
        "# print(data.x)\n",
        "# data = standardization(data)\n",
        "# print(data.x)\n",
        "# print(data.x.std(dim=0))\n",
        "# print(data.edge_index)\n",
        "# print(get_adjacency_matrix(data.edge_index, x.size(dim=0)).to_dense())\n",
        "# train, test = train_test_split(data, 0.2)\n",
        "# print(train.x)\n",
        "# print(train.edge_index)\n",
        "# print(test.x)\n",
        "# print(test.edge_index)"
      ],
      "metadata": {
        "id": "WAnNgCsXNMx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Sigma calculated above in node-level and edge-level DP case\n",
        "# gap = GAP(encoder, PMA(A, K_hop, sigma), Classification(K_hop, [60, 30, 20], [(K_hop+1)*20, 60, 30, num_classes]))\n",
        "# gap_model = gap.to(device)\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(gap_model.parameters(), lr=1e-1)\n",
        "\n",
        "# # if node_level:\n",
        "# #   optimizer = op.optimizers.optimizer.DPOptimizer(\n",
        "# #       # TODO: Fill out these parameters '?'\n",
        "# #       optimizer=optimizer,\n",
        "# #       noise_multiplier=?,\n",
        "# #       max_grad_norm=?,\n",
        "# #       loss_reduction='sum'\n",
        "# #   ) \n",
        "\n",
        "# epochs = 500\n",
        "# for t in range(epochs):\n",
        "#     # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "#     train(X, y, gap_model, loss_fn, optimizer)\n",
        "#     if t % 10 == 0:\n",
        "#       test(X, y, gap_model, loss_fn)\n",
        "# print(\"Done!\")"
      ],
      "metadata": {
        "id": "wrf0EaGHNQW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vb1hVZklFoDP",
        "Ty_dao27oEAB",
        "WebCohAOIX0P",
        "XDJP3FhOi_Ty",
        "BgX93DaHqqzh"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}