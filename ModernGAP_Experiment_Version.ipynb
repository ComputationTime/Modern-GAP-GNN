{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lua-Nova/Modern-GAP-GNN/blob/new/ModernGAP_Experiment_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aFQCNsDWU5mf"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# if torch.cuda.is_available():\n",
        "#   #NVIDIA GPU version\n",
        "#   %pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "#   %pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "#   %pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "#   %pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "#   %pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "# else:\n",
        "#   #CPU version\n",
        "#   %pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric==2.0.0 -f https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
        "# %pip uninstall pyvacy  --y\n",
        "# %pip install pyvacy\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fBxqGGgYU6k-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import Sequential, GCNConv\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb1hVZklFoDP"
      },
      "source": [
        "## Encoder Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1ScntAVtWL3M"
      },
      "outputs": [],
      "source": [
        "# create classes for layers that are used a lot to avoid repeating code\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  # e.g. dimensions = [50,40,30,20]\n",
        "    def __init__(self, dimensions):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        layers = []\n",
        "        for i in range(len(dimensions)-1):\n",
        "          layers.append(nn.Linear(dimensions[i], dimensions[i+1]))\n",
        "          layers.append(nn.SELU(inplace=True))\n",
        "\n",
        "        self.linear_selu_stack = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_selu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF264FWv0pJI"
      },
      "source": [
        "## PMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OMzLWqwF03Uu"
      },
      "outputs": [],
      "source": [
        "class AggregationModule(nn.Module):\n",
        "  edge_index = None\n",
        "\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "class PMA(AggregationModule):\n",
        "    # A - adjacency matrix     TODO: this should not be given to the module itself, it should access it in training (or from the graph dataset)\n",
        "    # num_hops - the number of hops covered by this GNN\n",
        "    def __init__(self, num_hops, sigma):\n",
        "        super().__init__()\n",
        "        self.num_hops = num_hops\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TEMP SOLUTION\n",
        "        if AggregationModule.edge_index is None:\n",
        "          raise RuntimeError(\"Set AggregationModule.edge_index [TEMP SOLUTION] before running\")\n",
        "        edge_index = AggregationModule.edge_index\n",
        "        A = get_adjacency_matrix(edge_index, x.size(dim=0))\n",
        "        out = [torch.nn.functional.normalize(x, dim=1)]\n",
        "        for k in range(self.num_hops):\n",
        "            aggr = torch.mm(torch.transpose(A, 0, 1), out[-1].to(device))\n",
        "            noised = aggr + torch.normal(torch.zeros(aggr.size()), std=self.sigma).to(device)\n",
        "            normalized = torch.nn.functional.normalize(noised, dim=1)\n",
        "            out.append(normalized)\n",
        "        return torch.stack(out)\n",
        "        # return torch.nn.functional.normalize(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PMAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PMAT(AggregationModule):\n",
        "    # def __init__(self, num_hops, transform_dimensions):\n",
        "    def __init__(self, num_hops, encoding_dimensions, sigma):\n",
        "        super().__init__()\n",
        "        self.num_hops = num_hops\n",
        "        self.sigma = sigma\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # self.transforms = nn.ModuleList()\n",
        "        self.attentions = nn.ModuleList()\n",
        "        for i in range(num_hops):\n",
        "          # self.transforms.append(nn.Linear(*transform_dimensions)) # Only 1 layer transformation\n",
        "          # self.attentions.append(MLP([2*transform_dimensions[-1], 1])) # Attention mechanism takes 2 encodings and outputs 1 weight\n",
        "          # TODO: Figure out if we want a transformer?\n",
        "          self.attentions.append(MLP([2*encoding_dimensions, 1]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TEMP SOLUTION\n",
        "        if AggregationModule.edge_index is None:\n",
        "          raise RuntimeError(\"Set AggregationModule.edge_index [TEMP SOLUTION] before running\")\n",
        "        edge_index = AggregationModule.edge_index\n",
        "        out = [torch.nn.functional.normalize(x, dim=1)]\n",
        "        for k in range(self.num_hops):\n",
        "            # Do we need to do a transform? I reckon we can use raw encoding and aggregate according to attention (and then the Classification module)\n",
        "            # can handle how the aggregations get transformed\n",
        "            # h = self.transforms[k](out[-1])\n",
        "            h = out[-1]\n",
        "            e_values = self.attentions[k](h[edge_index.T].reshape(edge_index.size(dim=1), 2*h.size(dim=1))) # DPSGD to guarantee DP attention training\n",
        "            # we have to use Sigmoid because if we use Softmax, removing an edge will change the weight of all other edges in the neighbourhood\n",
        "            alpha_values = self.sigmoid(e_values)\n",
        "            alpha = torch.sparse_coo_tensor(edge_index,\n",
        "                                            alpha_values.reshape(edge_index.size(dim=1)),\n",
        "                                            (x.size(dim=0), x.size(dim=0)),\n",
        "                                            dtype=torch.float).transpose(0, 1)\n",
        "\n",
        "            aggr = torch.sparse.mm(alpha, h)\n",
        "            # Might need to not use \"transforms\" and instead do raw aggregations like the original PMA\n",
        "            # aggr = torch.mm(self.A, out[-1]) \n",
        "            # noised = aggr # TODO: add noise # Gaussian mechanism to guarantee DP for neighbourhood aggregation\n",
        "            noised = aggr + torch.normal(torch.zeros(aggr.size()), std=self.sigma).to(device)\n",
        "            normalized = torch.nn.functional.normalize(noised, dim=1)\n",
        "            out.append(normalized)\n",
        "        return torch.stack(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PMWA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Similarity(nn.Module):\n",
        "    def __init__(self, function_name: str):\n",
        "        super().__init__()\n",
        "        if function_name == \"inner product\":\n",
        "            self.fun = lambda x : torch.sum(x[0, :]*x[1, :], dim = 1)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Not a valid similarity function\")\n",
        "            \n",
        "    def forward(self, x):\n",
        "        return self.fun(x)\n",
        "\n",
        "class PMWA(AggregationModule):\n",
        "    # def __init__(self, num_hops, transform_dimensions):\n",
        "    def __init__(self, num_hops, sigma, similarity_fun_name = \"inner product\"):\n",
        "        super().__init__()\n",
        "        self.num_hops = num_hops\n",
        "        self.sigma = sigma\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # self.transforms = nn.ModuleList()\n",
        "        self.similarity = Similarity(similarity_fun_name)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TEMP SOLUTION\n",
        "        if AggregationModule.edge_index is None:\n",
        "          raise RuntimeError(\"Set AggregationModule.edge_index [TEMP SOLUTION] before running\")\n",
        "        edge_index = AggregationModule.edge_index\n",
        "        out = [torch.nn.functional.normalize(x, dim=1)]\n",
        "        for k in range(self.num_hops):\n",
        "            # Do we need to do a transform? I reckon we can use raw encoding and aggregate according to attention (and then the Classification module)\n",
        "            # can handle how the aggregations get transformed\n",
        "            # h = self.transforms[k](out[-1])\n",
        "            h = out[-1]\n",
        "            e_values = self.similarity(h[edge_index])\n",
        "            # we have to use Sigmoid because if we use Softmax, removing an edge will change the weight of all other edges in the neighbourhood\n",
        "            alpha_values = self.sigmoid(e_values)\n",
        "            alpha = torch.sparse_coo_tensor(edge_index,\n",
        "                                            alpha_values.reshape(edge_index.size(dim=1)),\n",
        "                                            (x.size(dim=0), x.size(dim=0)),\n",
        "                                            dtype=torch.float).transpose(0, 1)\n",
        "\n",
        "            aggr = torch.sparse.mm(alpha, h)\n",
        "            # Might need to not use \"transforms\" and instead do raw aggregations like the original PMA\n",
        "            # aggr = torch.mm(self.A, out[-1]) \n",
        "            # noised = aggr # TODO: add noise # Gaussian mechanism to guarantee DP for neighbourhood aggregation\n",
        "            noised = aggr + torch.normal(torch.zeros(aggr.size()), std=self.sigma).to(device)\n",
        "            normalized = torch.nn.functional.normalize(noised, dim=1)\n",
        "            out.append(normalized)\n",
        "        return torch.stack(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty_dao27oEAB"
      },
      "source": [
        "## Classification Module\n",
        "NOTE: \n",
        "\n",
        "MLP base: The first MLP in the cassification module. \n",
        "\n",
        "MLP head: The last MLP and takes the combined output of all MLP base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7dLm3Q3HofT6"
      },
      "outputs": [],
      "source": [
        "class Classification(nn.Module):\n",
        "    # num_hops - the number of hops covered by this GNN\n",
        "    # encoder_dimensions - the MLP dimensions of each base MLP\n",
        "    # head_dimensions - the dimensions of the head MLP\n",
        "    def __init__(self, num_hops, encoder_dimensions, head_dimensions):\n",
        "        super().__init__()\n",
        "        self.base_mlps = nn.ModuleList()\n",
        "        self.num_hops = num_hops\n",
        "        if encoder_dimensions:\n",
        "          for i in range(num_hops+1):\n",
        "              self.base_mlps.append(MLP(encoder_dimensions))\n",
        "        self.head_mlp = MLP(head_dimensions) # TODO: should this be softmax? I think we add a softmax for classification tasks. We can test if it works better\n",
        "    \n",
        "    def forward(self, cache):\n",
        "        # forward through bases\n",
        "        out = []\n",
        "        for i in range(self.num_hops+1):\n",
        "          if self.base_mlps:\n",
        "            encoding = self.base_mlps[i](cache[i,:,:])\n",
        "            out.append(encoding) # add corresponding encoding\n",
        "          else:\n",
        "            out.append(cache[i, :, :])\n",
        "        # combine (use concatenation)\n",
        "        combined_x = torch.cat(out, dim=1)\n",
        "        # forward through head\n",
        "        return self.head_mlp(combined_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "evGOLmy-UrMs"
      },
      "outputs": [],
      "source": [
        "class GAP(nn.Module):\n",
        "  # encoder - pretrained encoder module\n",
        "  # pma - PMA module\n",
        "  # classification - classification module\n",
        "  def __init__(self, encoder, pma, classification): # TODO: decide whether we should recieve the models as parameters\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.encoder.requires_grad=False\n",
        "    self.pma = pma\n",
        "    self.classification = classification\n",
        "\n",
        "  def forward(self, x):\n",
        "    # initial node encoding\n",
        "    x_encoded = self.encoder(x)\n",
        "    # aggregation module\n",
        "    cache = self.pma(x_encoded) \n",
        "    # classification\n",
        "    return self.classification(cache) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WebCohAOIX0P"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Pf4P_bIZ5S",
        "outputId": "d327b660-d6b9-4fcd-cf5b-3048376d8089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epsilon: 4.00, Delta: 0.00, Sigma: 1.30\n"
          ]
        }
      ],
      "source": [
        "node_level = True\n",
        "\n",
        "# Edge level DP\n",
        "agg_epsilon, agg_delta, epsilon_1, epsilon_5, pmat_epsilon = 4, 1e-5, 4, 4, 4\n",
        "pmat_delta = agg_delta\n",
        "K_hop = 1\n",
        "agg_sigma = 1 / np.max(np.roots([K_hop/2, np.sqrt(2*K_hop*np.log(1/agg_delta)), -agg_epsilon]))\n",
        "# Node level DP\n",
        "if (node_level):\n",
        "  pass\n",
        "  # How do we calculate this?\n",
        "data = \"reddit\"\n",
        "batch_size = 256\n",
        "\n",
        "print(f\"Epsilon: {agg_epsilon:>0.2f}, Delta: {agg_delta:>0.2f}, Sigma: {agg_sigma:>0.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDJP3FhOi_Ty"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IPX-rb88ukrc"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "# this method partitions based on nodes (so edges between splits are not used)\n",
        "def train_test_split(dataset, test_ratio):\n",
        "    X, y, edge_index= dataset.x, dataset.y, dataset.edge_index\n",
        "    shuffle_ordering = torch.randperm(X.size(dim=0))\n",
        "\n",
        "    edge_mapping = torch.zeros(X.size(dim=0), dtype=torch.long)\n",
        "    edge_mapping[shuffle_ordering] = torch.arange(X.size(dim=0))\n",
        "\n",
        "    X = X[shuffle_ordering]\n",
        "    y = y[shuffle_ordering]\n",
        "    edge_index = edge_mapping[edge_index]\n",
        "\n",
        "    mask = torch.zeros(X.size(dim=0), dtype=torch.bool)\n",
        "    train_slice = int((1-test_ratio)*X.size(dim=0))\n",
        "    mask[:train_slice] = True\n",
        "\n",
        "    X_train = X[mask]\n",
        "    X_test = X[~mask]\n",
        "\n",
        "    y_train = y[mask]\n",
        "    y_test = y[~mask]\n",
        "\n",
        "    edge_index_train = edge_index[:, torch.logical_and(*mask[edge_index])]\n",
        "    edge_index_test = edge_index[:, torch.logical_and(*~mask[edge_index])] - train_slice\n",
        "\n",
        "    return Data(x=X_train, y=y_train, edge_index=edge_index_train), \\\n",
        "           Data(x=X_test, y=y_test, edge_index=edge_index_test)\n",
        "\n",
        "\n",
        "# returns filtered edge index, first removes edges that have removed src or dst nodes, then shifts indices of remained src/dst nodes\n",
        "def filter_edge_index(edge_index, filter):\n",
        "\n",
        "    node_indices = torch.arange(filter.size(dim=0))[filter]\n",
        "    edge_mapping = torch.zeros(filter.size(dim=0), dtype=torch.long)\n",
        "    edge_mapping[node_indices] = torch.arange(node_indices.size(dim=0))\n",
        "\n",
        "\n",
        "    edge_index = edge_index.to(torch.long)\n",
        "    edge_filter = torch.logical_and(*filter[edge_index])\n",
        "    return edge_mapping[edge_index[:, edge_filter]]\n",
        "\n",
        "def prepare_dataset(dataset, threshold):\n",
        "    X, y, edge_index = dataset.x, dataset.y, dataset.edge_index\n",
        "\n",
        "    # remove labels with less examples than threshold\n",
        "    index_map = torch.zeros(y.size())\n",
        "    included_classes = y.unique(return_counts=True)[1] >= threshold\n",
        "    filter = included_classes[y]\n",
        "    # remap labels (i.e. if they were 0-8 and we remove 4 labels, new labels should be between 0 and 4)\n",
        "    label_mapping = torch.zeros(included_classes.size(dim=0), dtype=torch.long)\n",
        "    label_mapping[included_classes] = torch.arange(torch.count_nonzero(included_classes))\n",
        "\n",
        "    y = label_mapping[y[filter]].to(torch.long)\n",
        "    X = X[filter]\n",
        "\n",
        "    # remove edges that had their nodes removed\n",
        "    edge_index = filter_edge_index(edge_index, filter)\n",
        "\n",
        "    return Data(x=X, y=y, edge_index=edge_index)\n",
        "\n",
        "# make sparse adjacency matrix, A\n",
        "def get_adjacency_matrix(edge_index, num_nodes):\n",
        "    values = torch.ones(edge_index.size(dim=1), dtype = torch.int).to(device)\n",
        "    A = torch.sparse_coo_tensor(edge_index, values, (num_nodes, num_nodes), dtype=torch.float)\n",
        "    return A\n",
        "\n",
        "def standardization(train_dataset, test_dataset):\n",
        "    X = train_dataset.x\n",
        "    means = X.mean(dim=0, keepdim=True)\n",
        "    stds = X.std(dim=0, keepdim=True)\n",
        "    X_train = (X - means) / stds\n",
        "    X_test = (test_dataset.x - means) / stds\n",
        "    return Data(x=X_train, y=train_dataset.y, edge_index=train_dataset.edge_index), Data(x=X_test, y=test_dataset.y, edge_index=test_dataset.edge_index)\n",
        "\n",
        "def add_self_edges(dataset):\n",
        "    X = dataset.x\n",
        "    self_edges = torch.stack((torch.arange(X.size(dim=0)), torch.arange(X.size(dim=0))))\n",
        "    edge_index = torch.cat((dataset.edge_index, self_edges), dim=1)\n",
        "    return Data(x=X, y=dataset.y, edge_index=edge_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7oEXJ63oyl-"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrAKSDBl03mk",
        "outputId": "52f0e8a1-b2c5-47cb-802c-399200ebd8a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29402180\n",
            "29402180\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import Reddit\n",
        "dataset = Reddit('.')[0]\n",
        "# prepare dataset by removing classes that have less than 1000 examples\n",
        "dataset = prepare_dataset(dataset, 10000)\n",
        "# dataset = add_self_edges(dataset)\n",
        "# get num classes\n",
        "num_classes = torch.unique(dataset.y).size(dim=0)\n",
        "\n",
        "# train/test split\n",
        "train_dataset, test_dataset = train_test_split(dataset, 0.2)\n",
        "print(train_dataset.edge_index.size(dim=1))\n",
        "# train_dataset = add_edge_to_low_degree_nodes(train_dataset, 10)\n",
        "print(train_dataset.edge_index.size(dim=1))\n",
        "# test_dataset = add_edge_to_low_degree_nodes(test_dataset, 10)\n",
        "# train_dataset, test_dataset = standardization(train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Facebook page-page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "29Swawd9we8f"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "X_train, y_train, edge_index_train = train_dataset.x, train_dataset.y, train_dataset.edge_index\n",
        "X_test, y_test, edge_index_test = test_dataset.x, test_dataset.y, test_dataset.edge_index\n",
        "\n",
        "# using large number like 10,000 so that all neighbours are sampled \n",
        "# I don't like how it samples, so I'm just gonna sample everything\n",
        "train_loader = NeighborLoader(train_dataset, num_neighbors=[X_train.size(dim=0)]*K_hop, \n",
        "                              batch_size=batch_size, shuffle=True)\n",
        "test_loader = NeighborLoader(test_dataset, num_neighbors=[X_test.size(dim=0)]*K_hop, \n",
        "                             batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4Md2Ds3q3CY"
      },
      "source": [
        "## Train/Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nu0AtfY7qzba"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "def train(batch, model, loss_fn, optimizer):\n",
        "  model.train()\n",
        "  X, y = batch.x.to(device), batch.y.to(device)\n",
        "  AggregationModule.edge_index = batch.edge_index.to(device)\n",
        "  # compute prediction error\n",
        "  pred = model(X)\n",
        "  loss = loss_fn(pred[:batch.batch_size], y[:batch.batch_size])\n",
        "  # backpropagation\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "# test batch\n",
        "def batch_test(batch, split, model, loss_fn, wordy=False):\n",
        "    size = batch.batch_size\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.inference_mode():\n",
        "        X, y = batch.x.to(device), batch.y.to(device)\n",
        "        AggregationModule.edge_index = batch.edge_index.to(device)\n",
        "        pred = model(X)\n",
        "        test_loss = loss_fn(pred[:batch.batch_size], y[:batch.batch_size]).item()\n",
        "        correct = (pred[:batch.batch_size].argmax(1) == y[:batch.batch_size]).type(torch.float).sum().item() / size\n",
        "    if wordy:\n",
        "      print(f\"{split.title()} Error: \\n Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f}\")\n",
        "    return test_loss, correct\n",
        "\n",
        "# test\n",
        "def test(loader, split, model, loss_fn):\n",
        "    size = len(loader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    for batch in loader:\n",
        "        batch_loss, batch_correct = batch_test(batch, split, model, loss_fn)\n",
        "        test_loss += batch_loss\n",
        "        correct += batch_correct\n",
        "    correct /= size\n",
        "    test_loss /= size\n",
        "    print(f\"{split.title()} Error: \\n Avg Accuracy: {(100*correct):>0.1f}%, Avg Loss: {test_loss:>8f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFvQEYa3j3O7"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XixvkAQpqcFM"
      },
      "source": [
        "Encoder Design\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9ZT6RQbkiTbj"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# encoder\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dimensions \u001b[39m=\u001b[39m [\u001b[39m602\u001b[39m, \u001b[39m300\u001b[39m, \u001b[39m60\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m encoder_model_reddit_PMA1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m      4\u001b[0m     MLP(dimensions),\n\u001b[1;32m      5\u001b[0m     nn\u001b[39m.\u001b[39mLinear(dimensions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], num_classes),\n\u001b[1;32m      6\u001b[0m     nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m encoder_model_reddit_PMA2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m      9\u001b[0m     MLP(dimensions),\n\u001b[1;32m     10\u001b[0m     nn\u001b[39m.\u001b[39mLinear(dimensions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], num_classes),\n\u001b[1;32m     11\u001b[0m     nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m encoder_model_reddit_PMA4 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     14\u001b[0m     MLP(dimensions),\n\u001b[1;32m     15\u001b[0m     nn\u001b[39m.\u001b[39mLinear(dimensions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], num_classes),\n\u001b[1;32m     16\u001b[0m     nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "# encoder\n",
        "dimensions = [602, 300, 60]\n",
        "encoder_model= nn.Sequential(\n",
        "    MLP(dimensions),\n",
        "    nn.Linear(dimensions[-1], num_classes),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Jk2-tbqLer"
      },
      "source": [
        "Encoder Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HYGNjZgqo7k",
        "outputId": "ebdcb08c-c809-4216-a3af-222270c6d91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Error: \n",
            " Accuracy: 80.1%, Loss: 1.469543\n",
            "Train Error: \n",
            " Accuracy: 82.0%, Loss: 1.458087\n",
            "Train Error: \n",
            " Accuracy: 82.2%, Loss: 1.452685\n",
            "Train Error: \n",
            " Accuracy: 85.4%, Loss: 1.426789\n",
            "Train Error: \n",
            " Accuracy: 86.3%, Loss: 1.420601\n",
            "Train Error: \n",
            " Accuracy: 84.2%, Loss: 1.434001\n",
            "Train Error: \n",
            " Accuracy: 82.8%, Loss: 1.446567\n",
            "Train Error: \n",
            " Accuracy: 83.0%, Loss: 1.445782\n",
            "Train Error: \n",
            " Accuracy: 84.8%, Loss: 1.427070\n",
            "Train Error: \n",
            " Accuracy: 80.7%, Loss: 1.467100\n",
            "Test Error: \n",
            " Avg Accuracy: 81.8%, Avg Loss: 1.456505\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "encoder_model = encoder_model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(encoder_model.parameters(), lr=1e-3)\n",
        "\n",
        "for t in range(200):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, encoder_model, loss_fn, optimizer)\n",
        "    if (t + 1) % 20 == 0:\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", encoder_model, loss_fn, True)\n",
        "test(test_loader, \"TEST\", encoder_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "encoder = encoder_model[0]\n",
        "encoder.requires_grad=False\n",
        "\n",
        "# for name, param in encoder_model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name, param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PMWA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOoJSnrTSVWZ"
      },
      "source": [
        "## PMAT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LKxUUyl_SW-i"
      },
      "outputs": [],
      "source": [
        "from pyvacy import optim, analysis\n",
        "# PMAT\n",
        "pmat_model = nn.Sequential(\n",
        "    encoder,\n",
        "    PMAT(K_hop, 60, agg_sigma),\n",
        "    Classification(K_hop, [], [(K_hop+1)*60, num_classes])\n",
        ")\n",
        "pmat_model = pmat_model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(encoder_model.parameters(), lr=1e-3)\n",
        "# TODO: Not allowed dyanmic batch_size for DPSGD, our batches are edge-wise\n",
        "# so they should have fixed batch_size!\n",
        "optimizer = optim.DPAdam(\n",
        "    l2_norm_clip=1.0,\n",
        "    noise_multiplier=1.0,\n",
        "    batch_size=batch_size,\n",
        "    params=pmat_model.parameters(),\n",
        "    lr=0.5e-1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpsiubMeSoTE",
        "outputId": "e7f1dc59-8f24-44ce-dc14-73e9c81a7d16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2.735276519360655\n",
            "Train Error: \n",
            " Accuracy: 92.6%, Loss: 0.296469\n",
            "Optimizer Achieves (0.2, 0.5)-DP\n",
            "LR: 0.05\n",
            "Epoch: 5.470553038721329\n",
            "Train Error: \n",
            " Accuracy: 93.2%, Loss: 0.291192\n",
            "Optimizer Achieves (0.3, 0.5)-DP\n",
            "LR: 0.05\n",
            "Epoch: 8.205829558082003\n",
            "Train Error: \n",
            " Accuracy: 96.1%, Loss: 0.155288\n",
            "Optimizer Achieves (0.4, 0.5)-DP\n",
            "LR: 0.025\n",
            "Epoch: 10.941106077442678\n",
            "Train Error: \n",
            " Accuracy: 92.6%, Loss: 0.293578\n",
            "Optimizer Achieves (0.4, 0.5)-DP\n",
            "LR: 0.025\n",
            "Epoch: 13.676382596803352\n",
            "Train Error: \n",
            " Accuracy: 93.6%, Loss: 0.249592\n",
            "Optimizer Achieves (0.5, 0.5)-DP\n",
            "LR: 0.0125\n",
            "Epoch: 16.411659116163893\n",
            "Train Error: \n",
            " Accuracy: 94.5%, Loss: 0.223149\n",
            "Optimizer Achieves (0.5, 0.5)-DP\n",
            "LR: 0.0125\n",
            "Epoch: 19.14693563552368\n",
            "Train Error: \n",
            " Accuracy: 94.5%, Loss: 0.226060\n",
            "Optimizer Achieves (0.6, 0.5)-DP\n",
            "LR: 0.00625\n",
            "Epoch: 21.882212154883465\n",
            "Train Error: \n",
            " Accuracy: 95.9%, Loss: 0.155581\n",
            "Optimizer Achieves (0.6, 0.5)-DP\n",
            "LR: 0.00625\n",
            "Epoch: 24.61748867424325\n",
            "Train Error: \n",
            " Accuracy: 93.6%, Loss: 0.235170\n",
            "Optimizer Achieves (0.7, 0.5)-DP\n",
            "LR: 0.003125\n",
            "Epoch: 27.352765193603037\n",
            "Train Error: \n",
            " Accuracy: 94.7%, Loss: 0.178354\n",
            "Optimizer Achieves (0.7, 0.5)-DP\n",
            "LR: 0.003125\n",
            "Epoch: 30.088041712962823\n",
            "Train Error: \n",
            " Accuracy: 95.7%, Loss: 0.176063\n",
            "Optimizer Achieves (0.8, 0.5)-DP\n",
            "LR: 0.0015625\n",
            "Epoch: 32.82331823232261\n",
            "Train Error: \n",
            " Accuracy: 95.9%, Loss: 0.150934\n",
            "Optimizer Achieves (0.8, 0.5)-DP\n",
            "LR: 0.0015625\n",
            "Epoch: 35.5585947516824\n",
            "Train Error: \n",
            " Accuracy: 94.9%, Loss: 0.192543\n",
            "Optimizer Achieves (0.9, 0.5)-DP\n",
            "LR: 0.00078125\n",
            "Epoch: 38.293871271042185\n",
            "Train Error: \n",
            " Accuracy: 94.9%, Loss: 0.177464\n",
            "Optimizer Achieves (0.9, 0.5)-DP\n",
            "LR: 0.00078125\n",
            "Epoch: 41.02914779040197\n",
            "Train Error: \n",
            " Accuracy: 95.1%, Loss: 0.200498\n",
            "Optimizer Achieves (0.9, 0.5)-DP\n",
            "LR: 0.000390625\n",
            "Epoch: 43.76442430976176\n",
            "Train Error: \n",
            " Accuracy: 95.7%, Loss: 0.141308\n",
            "Optimizer Achieves (1.0, 0.5)-DP\n",
            "LR: 0.000390625\n",
            "Epoch: 46.499700829121544\n",
            "Train Error: \n",
            " Accuracy: 95.3%, Loss: 0.170055\n",
            "Optimizer Achieves (1.0, 0.5)-DP\n",
            "LR: 0.0001953125\n",
            "Epoch: 49.23497734848133\n",
            "Train Error: \n",
            " Accuracy: 96.7%, Loss: 0.135052\n",
            "Optimizer Achieves (1.0, 0.5)-DP\n",
            "LR: 0.0001953125\n",
            "Epoch: 51.970253867841116\n",
            "Train Error: \n",
            " Accuracy: 94.5%, Loss: 0.211579\n",
            "Optimizer Achieves (1.1, 0.5)-DP\n",
            "LR: 9.765625e-05\n",
            "Epoch: 54.7055303872009\n",
            "Train Error: \n",
            " Accuracy: 93.6%, Loss: 0.222087\n",
            "Optimizer Achieves (1.1, 0.5)-DP\n",
            "LR: 9.765625e-05\n",
            "Test Error: \n",
            " Avg Accuracy: 87.5%, Avg Loss: 0.450456\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "edge_epochs = 0\n",
        "for t in range(5000):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, pmat_model, loss_fn, optimizer)\n",
        "    edge_epochs += batch_size / X_train.size(dim=0)\n",
        "    epsilon = analysis.moments_accountant(X_train.size(dim=0), batch_size, \n",
        "                                          1.0, edge_epochs, pmat_delta)\n",
        "    if (t + 1) % 500 == 0:\n",
        "      print(\"Epoch:\", edge_epochs)\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", pmat_model, loss_fn, True)\n",
        "      print(\"Optimizer Achieves ({:>0.1f}, {})-DP\".format(epsilon, pmat_delta))\n",
        "      print(\"LR:\", scheduler.get_last_lr()[0])\n",
        "    scheduler.step()\n",
        "    if epsilon >= pmat_epsilon:\n",
        "      break\n",
        "test(test_loader, \"TEST\", pmat_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "\n",
        "\n",
        "pmat = pmat_model[1]\n",
        "pmat.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqf6_kLMUZUn"
      },
      "source": [
        "## Full Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2os_fvuNJmcJ"
      },
      "source": [
        "TODO:\n",
        "  - try other dataset\n",
        "  - compare with their implementation\n",
        "  - try train/test split across edges\n",
        "  - batch normalization\n",
        "  - add identity matrix to see performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_SC_jUkl-y"
      },
      "source": [
        "Train full model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UPx__jwgLUCL"
      },
      "outputs": [],
      "source": [
        "model = GAP(encoder, \n",
        "            PMWA(K_hop, agg_sigma), \n",
        "            Classification(K_hop, [60, 20], [(K_hop+1)*20, num_classes]))\n",
        "# model = GAP(encoder, \n",
        "#             PMA(K_hop, agg_sigma), \n",
        "#             Classification(K_hop, [60, 20], [(K_hop+1)*20, num_classes]))\n",
        "# model = GAP(encoder, \n",
        "#             pmat, \n",
        "#             Classification(K_hop, [60, 20], [(K_hop+1)*20, num_classes]))\n",
        "model = model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z22WhxLcaveo",
        "outputId": "d695cdcd-4f0c-459d-cd37-608670a0a6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Error: \n",
            " Accuracy: 98.2%, Loss: 0.060551\n",
            "Train Error: \n",
            " Accuracy: 99.0%, Loss: 0.060459\n",
            "Train Error: \n",
            " Accuracy: 98.2%, Loss: 0.084095\n",
            "Train Error: \n",
            " Accuracy: 98.2%, Loss: 0.062814\n",
            "Train Error: \n",
            " Accuracy: 98.2%, Loss: 0.039232\n",
            "Train Error: \n",
            " Accuracy: 99.8%, Loss: 0.021454\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 11.99 GiB (GPU 0; 15.72 GiB total capacity; 483.12 MiB already allocated; 11.66 GiB free; 542.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[1;32m      2\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader))\n\u001b[0;32m----> 3\u001b[0m     train(batch, model, loss_fn, optimizer)\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m (t \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m       batch_test(\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader)), \u001b[39m\"\u001b[39m\u001b[39mTRAIN\u001b[39m\u001b[39m\"\u001b[39m, model, loss_fn, \u001b[39mTrue\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[17], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(batch, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# backpropagation\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.99 GiB (GPU 0; 15.72 GiB total capacity; 483.12 MiB already allocated; 11.66 GiB free; 542.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "for t in range(1000):\n",
        "    batch = next(iter(train_loader))\n",
        "    train(batch, model, loss_fn, optimizer)\n",
        "    if (t + 1) % 100 == 0:\n",
        "      batch_test(next(iter(train_loader)), \"TRAIN\", model, loss_fn, True)\n",
        "    scheduler.step()\n",
        "test(test_loader, \"TEST\", model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgX93DaHqqzh"
      },
      "source": [
        "## Backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-orS8HNbNcOc"
      },
      "outputs": [],
      "source": [
        "# TEMP CODE\n",
        "edge_index = torch.tensor([[0, 2, 0, 0, 2, 3, 1, 4, 1, 2, 4],\n",
        "                           [1, 0, 3, 4, 1, 1, 4, 4, 1, 3, 3]], dtype=torch.long)\n",
        "x = torch.tensor([\n",
        "    [0, 1, 0], \n",
        "    [1, 2, 2], \n",
        "    [2, 3, 1], \n",
        "    [3, 2, 4], \n",
        "    [4, 2, 3]], dtype=torch.float)\n",
        "y = torch.tensor([0, 1, 1, 2, 2], dtype=torch.long)\n",
        "pma = PMAT(2, 3, 0)\n",
        "AggregationModule.edge_index = edge_index\n",
        "stack = pma(x)\n",
        "print(stack)\n",
        "\n",
        "# # # plt.figure(figsize=(16,7))\n",
        "# # # plt.imshow(tensor)\n",
        "# # # plt.show()\n",
        "# #         # [encoder, pma, element_wise_mlp, combine, mlp]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JBYlKXUONVyK"
      },
      "outputs": [],
      "source": [
        "# from torch_geometric.datasets import Amazon\n",
        "\n",
        "# dataset = Amazon('.', name='Computers')[0]\n",
        "# # prepare dataset by removing classes that have less than 1000 examples\n",
        "# dataset = prepare_dataset(dataset, 1000)\n",
        "# dataset = add_self_edges(dataset)\n",
        "# # get num classes\n",
        "# num_classes = torch.unique(dataset.y).size(dim=0)\n",
        "\n",
        "# # train/test split\n",
        "# train_dataset, test_dataset = train_test_split(dataset, 0.2)\n",
        "# print(train_dataset.edge_index.size(dim=1))\n",
        "# train_dataset = add_edge_to_low_degree_nodes(train_dataset, 10)\n",
        "# print(train_dataset.edge_index.size(dim=1))\n",
        "# test_dataset = add_edge_to_low_degree_nodes(test_dataset, 10)\n",
        "# train_dataset, test_dataset = standardization(train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vof5FsrKqroZ"
      },
      "outputs": [],
      "source": [
        "# # train\n",
        "# def train(dataloader, model, loss_fn, optimizer, print_every = 100):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     model.train()\n",
        "#     for batch, (X, y) in enumerate(dataloader):\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "\n",
        "#         # Compute prediction error\n",
        "#         pred = model(X)\n",
        "#         loss = loss_fn(pred, y)\n",
        "\n",
        "#         # Backpropagation\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if batch % print_every == 0:\n",
        "#             loss, current = loss.item(), batch * len(X)\n",
        "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# # test\n",
        "# def test(dataloader, model, loss_fn):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     num_batches = len(dataloader)\n",
        "#     model.eval()\n",
        "#     test_loss, correct = 0, 0\n",
        "#     with torch.inference_mode():\n",
        "#         for X, y in dataloader:\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             pred = model(X)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= num_batches\n",
        "#     correct /= size\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAnNgCsXNMx3",
        "outputId": "a5064436-270d-4907-e2d2-063dc9144e32"
      },
      "outputs": [],
      "source": [
        "# from torch_geometric.loader import NeighborLoader\n",
        "# from torch_geometric.data import Data\n",
        "# edge_index = torch.tensor([[0, 2, 0, 0, 2, 3, 1, 4, 1, 2, 4],\n",
        "#                            [1, 0, 3, 4, 1, 1, 4, 4, 1, 3, 3]], dtype=torch.long)\n",
        "# x = torch.tensor([[0, 1, 0], [1, 2, 2], [2, 3, 1], [3, 2, 4], [4, 2, 3]], dtype=torch.float)\n",
        "# y = torch.tensor([0, 1, 1, 2, 2], dtype=torch.long)\n",
        "# data = Data(x=x, y=y, edge_index=edge_index)\n",
        "# loader = NeighborLoader(data, [10], batch_size=2, shuffle=True)\n",
        "# batch = next(iter(loader))\n",
        "# print(batch.x)\n",
        "# print(batch.edge_index)\n",
        "# print(batch.x[:batch.batch_size, ])\n",
        "# data = add_edge_to_low_degree_nodes(data, 3)\n",
        "# print(data.x)\n",
        "# data = standardization(data)\n",
        "# print(data.x)\n",
        "# print(data.x.std(dim=0))\n",
        "# print(data.edge_index)\n",
        "# print(get_adjacency_matrix(data.edge_index, x.size(dim=0)).to_dense())\n",
        "# train, test = train_test_split(data, 0.2)\n",
        "# print(train.x)\n",
        "# print(train.edge_index)\n",
        "# print(test.x)\n",
        "# print(test.edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wrf0EaGHNQW7"
      },
      "outputs": [],
      "source": [
        "# # Sigma calculated above in node-level and edge-level DP case\n",
        "# gap = GAP(encoder, PMA(A, K_hop, sigma), Classification(K_hop, [60, 30, 20], [(K_hop+1)*20, 60, 30, num_classes]))\n",
        "# gap_model = gap.to(device)\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.SGD(gap_model.parameters(), lr=1e-1)\n",
        "\n",
        "# # if node_level:\n",
        "# #   optimizer = op.optimizers.optimizer.DPOptimizer(\n",
        "# #       # TODO: Fill out these parameters '?'\n",
        "# #       optimizer=optimizer,\n",
        "# #       noise_multiplier=?,\n",
        "# #       max_grad_norm=?,\n",
        "# #       loss_reduction='sum'\n",
        "# #   ) \n",
        "\n",
        "# epochs = 500\n",
        "# for t in range(epochs):\n",
        "#     # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "#     train(X, y, gap_model, loss_fn, optimizer)\n",
        "#     if t % 10 == 0:\n",
        "#       test(X, y, gap_model, loss_fn)\n",
        "# print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vb1hVZklFoDP",
        "GF264FWv0pJI",
        "Ty_dao27oEAB",
        "WebCohAOIX0P",
        "XDJP3FhOi_Ty",
        "E7oEXJ63oyl-",
        "a4Md2Ds3q3CY",
        "WOoJSnrTSVWZ",
        "BgX93DaHqqzh"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
