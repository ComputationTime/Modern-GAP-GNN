{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lua-Nova/Modern-GAP-GNN/blob/main/ModernGAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  #NVIDIA GPU version\n",
        "\n",
        "  !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f f'https://data.pyg.org/whl/torch-1.12.0+{cutorch.version.cuda.replace('.','')}.html'\n",
        "else:\n",
        "  #CPU version\n",
        "  !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFQCNsDWU5mf",
        "outputId": "21fe7af5-98a6-4feb-de71-095b358d8266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcpu/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (286 kB)\n",
            "\u001b[K     |████████████████████████████████| 286 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcpu/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (641 kB)\n",
            "\u001b[K     |████████████████████████████████| 641 kB 36.9 MB/s \n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcpu/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 49.0 MB/s \n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcpu/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 46.1 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=f9adcb93c161f98e6c6c122cc54b70db821e494ebeeff85684d9525f3a4dc5cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-spline-conv, torch-sparse, torch-scatter, torch-geometric, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0 torch-geometric-2.1.0.post1 torch-scatter-2.0.9 torch-sparse-0.6.15 torch-spline-conv-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.nn import Sequential, GCNConv"
      ],
      "metadata": {
        "id": "fBxqGGgYU6k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Module"
      ],
      "metadata": {
        "id": "vb1hVZklFoDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(11)\n",
        "# create classes for layers that are used a lot to avoid repeating code\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  # e.g. dimensions = [50,40,30,20]\n",
        "    def __init__(self, dimensions):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        layers = []\n",
        "        for i in range(len(dimensions)-1):\n",
        "          layers.append(nn.Linear(dimensions[i], dimensions[i+1]))\n",
        "          layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "1ScntAVtWL3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PMA"
      ],
      "metadata": {
        "id": "GF264FWv0pJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PMA(nn.Module):\n",
        "    # A - adjacency matrix     TODO: this should not be given to the module itself, it should access it in training (or from the graph dataset)\n",
        "    # num_hops - the number of hops covered by this GNN\n",
        "    def __init__(self, A, num_hops):\n",
        "        super().__init__()\n",
        "        # TODO: Figure out if you should tranpose this\n",
        "        self.A_transpose = torch.transpose(A, 0,1)\n",
        "        self.num_hops = num_hops\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = [torch.nn.functional.normalize(x, dim=1)]\n",
        "        for k in range(self.num_hops):\n",
        "            aggr = torch.mm(self.A_transpose, out[-1])\n",
        "            # TODO: noise it up\n",
        "            noised = aggr\n",
        "            normalized = torch.nn.functional.normalize(noised, dim=1)\n",
        "            out.append(normalized)\n",
        "        return torch.stack(out)"
      ],
      "metadata": {
        "id": "OMzLWqwF03Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP CODE\n",
        "smoothing = 0.2\n",
        "A = torch.tensor([[1.,smoothing,smoothing],\n",
        "                  [smoothing,1.,smoothing],\n",
        "                  [smoothing,smoothing,1.]])\n",
        "x = torch.tensor([[1.,0.,0.],[0.,1.,0.],[0.,0.,1.]])\n",
        "pma = PMA(A, 10)\n",
        "tensor = pma(x)\n",
        "print(tensor.shape)\n",
        "tensor = tensor.cpu().numpy()\n",
        "\n",
        "# plt.figure(figsize=(16,7))\n",
        "# plt.imshow(tensor)\n",
        "# plt.show()\n",
        "        # [encoder, pma, element_wise_mlp, combine, mlp]\n"
      ],
      "metadata": {
        "id": "CZnwBQti_gK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fca8ae5-f561-4ff0-e551-3f4464276850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Module\n",
        "NOTE: \n",
        "\n",
        "MLP base: The first MLP in the cassification module. \n",
        "\n",
        "MLP head: The last MLP and takes the combined output of all MLP base."
      ],
      "metadata": {
        "id": "Ty_dao27oEAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classification(nn.Module):\n",
        "    # num_hops - the number of hops covered by this GNN\n",
        "    # encoder_dimensions - the MLP dimensions of each base MLP\n",
        "    # head_dimensions - the dimensions of the head MLP\n",
        "    def __init__(self, num_hops, encoder_dimensions, head_dimensions):\n",
        "        super().__init__()\n",
        "        self.base_mlps = nn.ModuleList()\n",
        "        for i in range(num_hops+1):\n",
        "          self.base_mlps.append(MLP(encoder_dimensions))\n",
        "        self.head_mlp = MLP(head_dimensions) # TODO: should this be softmax? I think we add a softmax for classification tasks. We can test if it works better\n",
        "    \n",
        "    def forward(self, cache):\n",
        "        # forward through bases\n",
        "        out = []\n",
        "        for i in range(len(self.base_mlps)):\n",
        "          encoding = self.base_mlps[i](cache[i,:,:])\n",
        "          out.append(encoding) # add corresponding encoding\n",
        "        # combine (use concatenation)\n",
        "        combined_x = torch.cat(out, dim=1)\n",
        "        # forward through head\n",
        "        return self.head_mlp(combined_x)"
      ],
      "metadata": {
        "id": "7dLm3Q3HofT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAP(nn.Module):\n",
        "  # encoder - pretrained encoder module\n",
        "  # pma - PMA module\n",
        "  # classification - classification module\n",
        "  def __init__(self, encoder, pma, classification): # TODO: decide whether we should recieve the models as parameters\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.pma = pma\n",
        "    self.classification = classification\n",
        "\n",
        "  def forward(self, x):\n",
        "    # initial node encoding\n",
        "    x_encoded = self.encoder(x)\n",
        "    # aggregation module\n",
        "    cache = self.pma(x_encoded) \n",
        "    # classification\n",
        "    return self.classification(cache) \n"
      ],
      "metadata": {
        "id": "evGOLmy-UrMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Test\n"
      ],
      "metadata": {
        "id": "a4Md2Ds3q3CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "def train(dataset, model, loss_fn, optimizer): \n",
        "    # make this into dataloader using backup\n",
        "    size = dataset['x'].size()[1]\n",
        "    model.train()\n",
        "    X, y = dataset['x'], dataset['y']\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# test\n",
        "def test(dataset, model, loss_fn):\n",
        "    size = dataset['x'].size()[1]\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.inference_mode():\n",
        "        X, y = dataset['x'], dataset['y']\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        test_loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "nu0AtfY7qzba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "E7oEXJ63oyl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import KarateClub\n",
        "from torch_geometric.loader import DataLoader\n",
        "dataset = KarateClub()[0]\n",
        "num_classes = torch.unique(dataset['y']).size()[0]\n",
        "# loader = DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n",
        "A = torch.zeros((dataset['x'].size()[1], dataset['x'].size()[1]), dtype=torch.float)\n",
        "# since we are using an adjacency matrix instead of edgelist, make that\n",
        "for i in range(dataset['edge_index'].size()[1]):\n",
        "  src, dst = dataset['edge_index'][0, i], dataset['edge_index'][1, i]\n",
        "  # since undirected\n",
        "  A[src, dst] = 1\n",
        "  A[dst, src] = 1"
      ],
      "metadata": {
        "id": "wXnUD6sXo3M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Reddit\n",
        "from torch_geometric.loader import DataLoader\n",
        "dataset = Reddit('.')\n",
        "# throw away classes with less than 10k nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aViLSsl2VD9K",
        "outputId": "14a531a3-12b6-4489-ea39-d1b5a6eca3e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(30)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataset.data['y']\n",
        "y_per_class = []\n",
        "mask_per_class = []\n",
        "for label in range(dataset.num_classes):\n",
        "    mask = y == label\n",
        "    mask_per_class.append(mask)\n",
        "    y_per_class.append(y[mask])\n",
        "    print(label, y_per_class[label].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nidf7qRK5ge",
        "outputId": "eea34173-0dcb-4f5a-8c88-250c50ed89e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([13101])\n",
            "1 torch.Size([3550])\n",
            "2 torch.Size([3302])\n",
            "3 torch.Size([15181])\n",
            "4 torch.Size([2322])\n",
            "5 torch.Size([3597])\n",
            "6 torch.Size([3952])\n",
            "7 torch.Size([2138])\n",
            "8 torch.Size([11187])\n",
            "9 torch.Size([2246])\n",
            "10 torch.Size([4928])\n",
            "11 torch.Size([2964])\n",
            "12 torch.Size([1696])\n",
            "13 torch.Size([2731])\n",
            "14 torch.Size([4854])\n",
            "15 torch.Size([28272])\n",
            "16 torch.Size([1003])\n",
            "17 torch.Size([2639])\n",
            "18 torch.Size([13999])\n",
            "19 torch.Size([10308])\n",
            "20 torch.Size([1596])\n",
            "21 torch.Size([4066])\n",
            "22 torch.Size([8222])\n",
            "23 torch.Size([12146])\n",
            "24 torch.Size([328])\n",
            "25 torch.Size([1659])\n",
            "26 torch.Size([4239])\n",
            "27 torch.Size([5962])\n",
            "28 torch.Size([4673])\n",
            "29 torch.Size([5101])\n",
            "30 torch.Size([2846])\n",
            "31 torch.Size([4570])\n",
            "32 torch.Size([1575])\n",
            "33 torch.Size([4960])\n",
            "34 torch.Size([3429])\n",
            "35 torch.Size([4202])\n",
            "36 torch.Size([4180])\n",
            "37 torch.Size([4233])\n",
            "38 torch.Size([12797])\n",
            "39 torch.Size([3099])\n",
            "40 torch.Size([5112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "SFvQEYa3j3O7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Design\n"
      ],
      "metadata": {
        "id": "XixvkAQpqcFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder\n",
        "dimensions = [34, 17, 5]\n",
        "encoder_train = nn.Sequential(\n",
        "    MLP(dimensions),\n",
        "    nn.Linear(dimensions[-1],num_classes),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ],
      "metadata": {
        "id": "9ZT6RQbkiTbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Pretraining"
      ],
      "metadata": {
        "id": "B8Jk2-tbqLer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = encoder_train.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(encoder_model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(dataset, encoder_model, loss_fn, optimizer)\n",
        "test(dataset, encoder_model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "encoder = encoder_model[0]\n",
        "\n",
        "# for name, param in encoder_model.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         print(name, param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HYGNjZgqo7k",
        "outputId": "29f4f4f1-448e-4bdf-9703-cdf72661f364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 35.3%, Avg loss: 1.383978 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train full model"
      ],
      "metadata": {
        "id": "Lo_SC_jUkl-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gap = GAP(encoder, PMA(A, 5), Classification(5, [5, 5, 4], [24, 12, 4]))\n",
        "gap_model = gap.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(gap_model.parameters(), lr=1e-4)\n",
        "\n",
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    # print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(dataset, gap_model, loss_fn, optimizer)\n",
        "test(dataset, gap_model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0yD8RqskkhV",
        "outputId": "4579c323-9d40-4fcd-b883-721b30419079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: \n",
            " Accuracy: 38.2%, Avg loss: 1.364594 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backup"
      ],
      "metadata": {
        "id": "BgX93DaHqqzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # train\n",
        "# def train(dataloader, model, loss_fn, optimizer, print_every = 100):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     model.train()\n",
        "#     for batch, (X, y) in enumerate(dataloader):\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "\n",
        "#         # Compute prediction error\n",
        "#         pred = model(X)\n",
        "#         loss = loss_fn(pred, y)\n",
        "\n",
        "#         # Backpropagation\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if batch % print_every == 0:\n",
        "#             loss, current = loss.item(), batch * len(X)\n",
        "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# # test\n",
        "# def test(dataloader, model, loss_fn):\n",
        "#     size = len(dataloader.dataset)\n",
        "#     num_batches = len(dataloader)\n",
        "#     model.eval()\n",
        "#     test_loss, correct = 0, 0\n",
        "#     with torch.inference_mode():\n",
        "#         for X, y in dataloader:\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             pred = model(X)\n",
        "#             test_loss += loss_fn(pred, y).item()\n",
        "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "#     test_loss /= num_batches\n",
        "#     correct /= size\n",
        "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "vof5FsrKqroZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}